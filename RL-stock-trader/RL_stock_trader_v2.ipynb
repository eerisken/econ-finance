{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP__RqgWeIpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14318dd-abe9-45ff-e0d8-537f88301cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/187.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra] sb3-contrib gymnasium --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_adjusted_price(symbol, df):\n",
        "    # Step 1: Start with a factor column = 1.0\n",
        "    df['AdjFactor'] = 1.0\n",
        "\n",
        "    # Step 2: Apply stock splits (backward adjustment)\n",
        "    for i in range(len(df)):\n",
        "        split = df['Stock Splits'].iloc[i]\n",
        "        if split != 0:  # e.g., 2.0 for 2-for-1\n",
        "            ratio = 1.0 / split\n",
        "            df.loc[:df.index[i], 'AdjFactor'] *= ratio\n",
        "\n",
        "    # Step 3: Apply dividends (backward adjustment)\n",
        "    # We scale all earlier prices to simulate reinvestment\n",
        "    for i in range(len(df)):\n",
        "        dividend = df['Dividends'].iloc[i]\n",
        "        if dividend != 0:\n",
        "            close_price = df['Close'].iloc[i]\n",
        "            ratio = (close_price - dividend) / close_price\n",
        "            df.loc[:df.index[i], 'AdjFactor'] *= ratio\n",
        "\n",
        "    # Step 4: Adjusted Close = Close * cumulative adjustment factor\n",
        "    df['AdjClose'] = df['Close'] * df['AdjFactor']\n",
        "    df['AdjVolume'] = df['Volume'] * df['AdjFactor']\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "FtHRSUGYeT6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "stock = yf.Ticker(\"KCHOL.IS\")\n",
        "df_stock = stock.history(start=\"2010-01-04\", end=\"2025-09-1\")\n",
        "df_stock = df_stock.iloc[1:]\n",
        "df_stock.index = df_stock.index.date\n",
        "df_stock = calculate_adjusted_price(\"KCHOL.IS\", df_stock)\n",
        "df_stock.rename(columns={\"Volume\": \"KOC_Volume\"}, inplace=True)\n",
        "df_stock.rename(columns={\"AdjClose\": \"KOC_AdjClose\"}, inplace=True)\n",
        "\n",
        "sahol = yf.Ticker(\"SAHOL.IS\")\n",
        "df_sahol = sahol.history(start=\"2010-01-04\", end=\"2025-09-1\")\n",
        "df_sahol = df_sahol.iloc[1:]\n",
        "df_sahol.index = df_sahol.index.date\n",
        "df_sahol = calculate_adjusted_price(\"SAHOL.IS\", df_sahol)\n",
        "df_sahol.rename(columns={\"Volume\": \"SAHOL_Volume\"}, inplace=True)\n",
        "df_sahol.rename(columns={\"AdjClose\": \"SAHOL_AdjClose\"}, inplace=True)\n",
        "\n",
        "eregli = yf.Ticker(\"EREGL.IS\")\n",
        "df_eregli = eregli.history(start=\"2010-01-04\", end=\"2025-09-1\")\n",
        "df_eregli = df_eregli.iloc[1:]\n",
        "df_eregli.index = df_eregli.index.date\n",
        "df_eregli = calculate_adjusted_price(\"EREGL.IS\", df_eregli)\n",
        "df_eregli.rename(columns={\"Volume\": \"EREGL_Volume\"}, inplace=True)\n",
        "df_eregli.rename(columns={\"AdjClose\": \"EREGL_AdjClose\"}, inplace=True)\n",
        "\n",
        "# === Download XU100 index (BIST 100) - Yahoo Finance ticker for BIST 100 is \"XU100.IS\"\n",
        "xu100 = yf.Ticker(\"XU100.IS\")\n",
        "df_xu100 = xu100.history(start=\"2010-01-04\", end=\"2025-09-1\")[[\"Close\"]]\n",
        "df_xu100.rename(columns={\"Close\": \"XU100_Close\"}, inplace=True)\n",
        "df_xu100 = df_xu100.iloc[1:]\n",
        "df_xu100.index = df_xu100.index.date\n",
        "\n",
        "# === Download USD/TRY exchange rate ===\n",
        "usdtry = yf.Ticker(\"USDTRY=X\")\n",
        "df_usdtry = usdtry.history(start=\"2010-01-04\", end=\"2025-09-1\")[[\"Close\"]]\n",
        "df_usdtry.rename(columns={\"Close\": \"USDTRY_Close\"}, inplace=True)\n",
        "\n",
        "df_stock.index = pd.to_datetime(df_stock.index).tz_localize(None)\n",
        "df_sahol.index = pd.to_datetime(df_sahol.index).tz_localize(None)\n",
        "df_eregli.index = pd.to_datetime(df_eregli.index).tz_localize(None)\n",
        "df_xu100.index = pd.to_datetime(df_xu100.index).tz_localize(None)\n",
        "df_usdtry.index = pd.to_datetime(df_usdtry.index).tz_localize(None)\n",
        "\n",
        "# === Merge all datasets on Date ===\n",
        "df = (\n",
        "    df_stock\n",
        "    .join(df_sahol, how=\"left\", rsuffix=\"_sahol\")\n",
        "    .join(df_eregli, how=\"left\", rsuffix=\"_eregli\")\n",
        "    .join(df_xu100, how=\"left\", rsuffix=\"_xu100\")\n",
        "    .join(df_usdtry, how=\"left\", rsuffix=\"_usdtry\")\n",
        ")\n",
        "df = df[[\"EREGL_Volume\", \"EREGL_AdjClose\", \"SAHOL_Volume\", \"SAHOL_AdjClose\", \"KOC_Volume\", \"KOC_AdjClose\", \"XU100_Close\", \"USDTRY_Close\"]]\n",
        "\n",
        "# === Forward fill missing values (holidays, weekends) ===\n",
        "df.fillna(method=\"ffill\", inplace=True)\n",
        "\n",
        "# === Save to CSV for reuse ===\n",
        "df.to_csv(\"assets.csv\")\n",
        "\n",
        "print(df.head())\n",
        "print(df.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw-1w4ceYsLo",
        "outputId": "8e3de69e-1430-4ae8-8815-08965e5553b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            EREGL_Volume  EREGL_AdjClose  SAHOL_Volume  SAHOL_AdjClose  \\\n",
            "2010-01-05      73491364        0.013520       5058266        2.207235   \n",
            "2010-01-06      27708427        0.013580       4992228        2.169179   \n",
            "2010-01-07      40402260        0.013342       2732948        2.169179   \n",
            "2010-01-08      46893994        0.013342      20476268        2.226263   \n",
            "2010-01-11      68805135        0.013520       8156529        2.169179   \n",
            "\n",
            "            KOC_Volume  KOC_AdjClose  XU100_Close  USDTRY_Close  \n",
            "2010-01-05    11165434      1.726188   541.148132        1.4727  \n",
            "2010-01-06     3897700      1.733472   545.471130        1.4714  \n",
            "2010-01-07     3662297      1.726188   549.726074        1.4715  \n",
            "2010-01-08     5993209      1.726188   547.976074        1.4580  \n",
            "2010-01-11     3878719      1.675204   539.208130        1.4502  \n",
            "            EREGL_Volume  EREGL_AdjClose  SAHOL_Volume  SAHOL_AdjClose  \\\n",
            "2025-08-25     187416068       29.660000      24798986       98.650002   \n",
            "2025-08-26     326760874       29.680000      30181629       98.050003   \n",
            "2025-08-27     169070592       29.580000      19201794       96.099998   \n",
            "2025-08-28     174066726       29.799999      14599534       95.949997   \n",
            "2025-08-29     182500594       29.860001      24042723       94.199997   \n",
            "\n",
            "            KOC_Volume  KOC_AdjClose   XU100_Close  USDTRY_Close  \n",
            "2025-08-25    21759418    189.100006  11477.799805     40.960300  \n",
            "2025-08-26    22895547    190.399994  11529.799805     41.003502  \n",
            "2025-08-27    15463054    185.800003  11359.000000     41.037498  \n",
            "2025-08-28    19216001    184.800003  11368.799805     41.051899  \n",
            "2025-08-29    26149001    183.300003  11288.099609     41.140499  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3934803299.py:57: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df.fillna(method=\"ffill\", inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2vTW4hAV0Jn",
        "outputId": "33fd2e61-13e7-4692-e3fa-4fdac4ef7ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            EREGL_Volume  EREGL_AdjClose  SAHOL_Volume  SAHOL_AdjClose  \\\n",
            "2010-01-05      73491364        0.013520       5058266        2.207236   \n",
            "2010-01-06      27708427        0.013580       4992228        2.169180   \n",
            "2010-01-07      40402260        0.013342       2732948        2.169180   \n",
            "2010-01-08      46893994        0.013342      20476268        2.226263   \n",
            "2010-01-11      68805135        0.013520       8156529        2.169180   \n",
            "\n",
            "            KOC_Volume  KOC_AdjClose  XU100_Close  USDTRY_Close  \n",
            "2010-01-05    11165434      1.726188   541.148132        1.4727  \n",
            "2010-01-06     3897700      1.733472   545.471130        1.4714  \n",
            "2010-01-07     3662297      1.726188   549.726074        1.4715  \n",
            "2010-01-08     5993209      1.726188   547.976074        1.4580  \n",
            "2010-01-11     3878719      1.675204   539.208130        1.4502  \n",
            "            EREGL_Volume  EREGL_AdjClose  SAHOL_Volume  SAHOL_AdjClose  \\\n",
            "2025-08-25     187416068       29.660000      24798986       98.650002   \n",
            "2025-08-26     326760874       29.680000      30181629       98.050003   \n",
            "2025-08-27     169070592       29.580000      19201794       96.099998   \n",
            "2025-08-28     174066726       29.799999      14599534       95.949997   \n",
            "2025-08-29     182500594       29.860001      24042723       94.199997   \n",
            "\n",
            "            KOC_Volume  KOC_AdjClose   XU100_Close  USDTRY_Close  \n",
            "2025-08-25    21759418    189.100006  11477.799805     40.960300  \n",
            "2025-08-26    22895547    190.399994  11529.799805     41.003502  \n",
            "2025-08-27    15463054    185.800003  11359.000000     41.037498  \n",
            "2025-08-28    19216001    184.800003  11368.799805     41.051899  \n",
            "2025-08-29    26149001    183.300003  11288.099609     41.140499  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Split Data\n",
        "# ===============================\n",
        "train_df = df.loc[(df.index < df.index[int(len(df)*0.75)])]  #\n",
        "val_df   = df.loc[(df.index >= df.index[int(len(df)*0.75)]) & (df.index < df.index[int(len(df)*0.85)])]\n",
        "test_df  = df.loc[(df.index >= df.index[int(len(df)*0.85)])]\n",
        "\n",
        "print(\"Train:\", train_df.shape, \"Val:\", val_df.shape, \"Test:\", test_df.shape)\n",
        "\n",
        "episode_length = len(train_df)\n",
        "num_episodes = 50   # 50 replays of dataset\n",
        "\n",
        "# Compute total timesteps\n",
        "total_timesteps = episode_length * num_episodes\n",
        "\n",
        "print(f\"Episode length: {episode_length} steps\")\n",
        "print(f\"Training for {num_episodes} episodes\")\n",
        "print(f\"Total timesteps = {total_timesteps}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0tYLGRDee4v",
        "outputId": "84899362-5c42-4082-f20d-fe072b97a556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (3015, 8) Val: (402, 8) Test: (603, 8)\n",
            "Episode length: 3015 steps\n",
            "Training for 50 episodes\n",
            "Total timesteps = 150750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from sb3_contrib import MaskablePPO\n",
        "from sb3_contrib.common.wrappers import ActionMasker\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from typing import Optional, Any, Dict\n",
        "\n",
        "\n",
        "class MultiAssetTradingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Multi-Asset Trading Environment for Stable-Baselines3 with Gymnasium:\n",
        "    - You can only hold one asset at a time (or be in cash)\n",
        "    - Uses action masking to enforce valid actions based on current state\n",
        "    - Enhanced with portfolio state features and risk metrics\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 1}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df,\n",
        "        window_size: int = 30,\n",
        "        initial_balance: float = 10_000.0,\n",
        "        fee_pct: float = 0.001,\n",
        "        normalize_obs: bool = True,\n",
        "        render_mode: Optional[str] = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.assets = [\"EREGL_AdjClose\", \"SAHOL_AdjClose\", \"KOC_AdjClose\"]\n",
        "        self.other_features = [\"EREGL_Volume\", \"SAHOL_Volume\", \"KOC_Volume\", \"XU100_Close\", \"USDTRY_Close\"]\n",
        "        self.n_assets = len(self.assets)\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.initial_balance = float(initial_balance)\n",
        "        self.fee_pct = float(fee_pct)\n",
        "        self.normalize_obs = normalize_obs\n",
        "\n",
        "        # Store original data for normalization\n",
        "        if self.normalize_obs:\n",
        "            self.asset_means = self.df[self.assets].mean().values\n",
        "            self.asset_stds = self.df[self.assets].std().values\n",
        "            # Avoid division by zero\n",
        "            self.asset_stds = np.maximum(self.asset_stds, 1e-8)\n",
        "\n",
        "        # Enhanced observation space: prices + portfolio state\n",
        "        obs_features = len(self.assets) + len(self.other_features)\n",
        "        if normalize_obs:\n",
        "            obs_features += 4  # cash_ratio, current_asset_onehot (3 values), portfolio_ratio\n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(window_size * obs_features,),  # Flattened for SB3\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Action space:\n",
        "        # 0 = Hold\n",
        "        # 1 = Buy Koç, 2 = Sell Koç\n",
        "        # 3 = Buy XU100, 4 = Sell XU100\n",
        "        # 5 = Buy USD, 6 = Sell USD\n",
        "        self.action_space = spaces.Discrete(1 + 2 * self.n_assets)\n",
        "\n",
        "        # Track performance metrics\n",
        "        self.episode_trades = 0\n",
        "        self.episode_fees = 0.0\n",
        "        self.max_portfolio_value = 0.0\n",
        "        self.portfolio_history = []\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"Reset environment - Gymnasium style\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.balance = self.initial_balance\n",
        "        self.holdings = np.zeros(self.n_assets)\n",
        "        self.current_step = self.window_size\n",
        "        self.current_asset = None  # None means in cash\n",
        "\n",
        "        # Reset episode metrics\n",
        "        self.episode_trades = 0\n",
        "        self.episode_fees = 0.0\n",
        "        self.max_portfolio_value = self.initial_balance\n",
        "        self.portfolio_history = [self.initial_balance]\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = {\n",
        "            \"portfolio_value\": self.initial_balance,\n",
        "            \"episode_trades\": 0,\n",
        "            \"episode_fees\": 0.0,\n",
        "            \"cash\": self.balance,\n",
        "            \"holdings\": self.holdings.copy(),\n",
        "            \"current_asset\": self.current_asset\n",
        "        }\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"Get observation - flattened for SB3\"\"\"\n",
        "        # Get price window\n",
        "        start_idx = max(0, self.current_step - self.window_size)\n",
        "        end_idx = self.current_step\n",
        "\n",
        "        price_window = self.df.loc[start_idx:end_idx-1, self.assets].values\n",
        "        other_features_window = self.df.loc[start_idx:end_idx-1, self.other_features].values\n",
        "\n",
        "        # Ensure we have the right window size (pad if necessary at the beginning)\n",
        "        if price_window.shape[0] < self.window_size:\n",
        "            padding = np.tile(price_window[0:1], (self.window_size - price_window.shape[0], 1))\n",
        "            price_window = np.vstack([padding, price_window])\n",
        "\n",
        "        if self.normalize_obs:\n",
        "            # Normalize prices\n",
        "            price_window = (price_window - self.asset_means) / self.asset_stds\n",
        "\n",
        "            # Add portfolio state features\n",
        "            current_prices = self.df.loc[self.current_step, self.assets].values\n",
        "            portfolio_value = self._get_portfolio_value(current_prices)\n",
        "\n",
        "            # Portfolio state features\n",
        "            #cash_ratio = self.balance / portfolio_value if portfolio_value > 0 else 0\n",
        "\n",
        "            # One-hot encoding for current asset (or [0,0,0] if in cash)\n",
        "            asset_onehot = np.zeros(self.n_assets)\n",
        "            if self.current_asset is not None:\n",
        "                asset_onehot[self.current_asset] = 1.0\n",
        "\n",
        "            portfolio_ratio = portfolio_value / self.initial_balance\n",
        "\n",
        "            # Combine all state features\n",
        "            #state_features = np.concatenate([[cash_ratio], asset_onehot, [portfolio_ratio]])\n",
        "            state_features = np.concatenate([asset_onehot, [portfolio_ratio]])\n",
        "\n",
        "            # Repeat state features for each time step and concatenate with prices\n",
        "            state_window = np.tile(state_features, (self.window_size, 1))\n",
        "            obs_window = np.concatenate([price_window, other_features_window, state_window], axis=1)\n",
        "        else:\n",
        "            obs_window = price_window\n",
        "\n",
        "        return obs_window.flatten().astype(np.float32)\n",
        "\n",
        "    def action_masks(self):\n",
        "        \"\"\"Return valid action mask for SB3 MaskablePPO\"\"\"\n",
        "        mask = np.zeros(self.action_space.n, dtype=bool)\n",
        "\n",
        "        # Hold is always valid\n",
        "        mask[0] = True\n",
        "\n",
        "        if self.current_asset is None:\n",
        "            # In cash -> only buys are valid (and hold)\n",
        "            for i in range(self.n_assets):\n",
        "                mask[1 + 2 * i] = True  # buy_i\n",
        "        else:\n",
        "            # Holding an asset -> only sell that asset is valid (and hold)\n",
        "            idx = self.current_asset\n",
        "            mask[2 + 2 * idx] = True  # sell_idx\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Step function for SB3 with Gymnasium interface\"\"\"\n",
        "        # Check if action is valid (for debugging)\n",
        "        if not self.action_masks()[action]:\n",
        "            print(f\"Warning: Invalid action {action} taken. Current asset: {self.current_asset}\")\n",
        "\n",
        "        price_vec = self.df.loc[self.current_step, self.assets].values\n",
        "        prev_portfolio_value = self._get_portfolio_value(price_vec)\n",
        "\n",
        "        executed = \"hold\"\n",
        "        fee_paid = 0.0\n",
        "        trade_executed = False\n",
        "\n",
        "        # Process action\n",
        "        if action == 0:  # Hold\n",
        "            pass\n",
        "        elif action % 2 == 1:  # Buy (odd numbers)\n",
        "            asset_idx = (action - 1) // 2\n",
        "            if self.current_asset is None and self.balance > 0:\n",
        "                price = price_vec[asset_idx]\n",
        "                fee_paid = self.balance * self.fee_pct\n",
        "                self.holdings[asset_idx] = (self.balance - fee_paid) / price\n",
        "                self.balance = 0.0\n",
        "                self.current_asset = asset_idx\n",
        "                executed = f\"buy_{self.assets[asset_idx]}\"\n",
        "                trade_executed = True\n",
        "        elif action % 2 == 0 and action > 0:  # Sell (even numbers > 0)\n",
        "            asset_idx = (action - 2) // 2\n",
        "            if self.current_asset == asset_idx:\n",
        "                price = price_vec[asset_idx]\n",
        "                notional = self.holdings[asset_idx] * price\n",
        "                fee_paid = notional * self.fee_pct\n",
        "                self.balance += notional - fee_paid\n",
        "                self.holdings[asset_idx] = 0.0\n",
        "                self.current_asset = None\n",
        "                executed = f\"sell_{self.assets[asset_idx]}\"\n",
        "                trade_executed = True\n",
        "\n",
        "        # Update metrics\n",
        "        if trade_executed:\n",
        "            self.episode_trades += 1\n",
        "            self.episode_fees += fee_paid\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= len(self.df) - 1\n",
        "        truncated = False\n",
        "\n",
        "        # Calculate new portfolio value and reward\n",
        "        price_vec_next = self.df.loc[self.current_step, self.assets].values\n",
        "        portfolio_value = self._get_portfolio_value(price_vec_next)\n",
        "        self.portfolio_history.append(portfolio_value)\n",
        "\n",
        "        # Update max portfolio value for drawdown calculation\n",
        "        self.max_portfolio_value = max(self.max_portfolio_value, portfolio_value)\n",
        "\n",
        "        # Enhanced reward function\n",
        "        reward = self._calculate_reward(prev_portfolio_value, portfolio_value, fee_paid)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = {\n",
        "            \"portfolio_value\": portfolio_value,\n",
        "            \"executed\": executed,\n",
        "            \"fee_paid\": fee_paid,\n",
        "            \"holdings\": self.holdings.copy(),\n",
        "            \"cash\": self.balance,\n",
        "            \"episode_trades\": self.episode_trades,\n",
        "            \"episode_fees\": self.episode_fees,\n",
        "            \"drawdown\": (self.max_portfolio_value - portfolio_value) / self.max_portfolio_value,\n",
        "            \"return\": (portfolio_value - self.initial_balance) / self.initial_balance,\n",
        "            \"current_asset\": self.current_asset\n",
        "        }\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _calculate_reward(self, prev_value, current_value, fee_paid):\n",
        "        \"\"\"Enhanced reward function with risk adjustment and fee penalty\"\"\"\n",
        "        # Basic return\n",
        "        raw_return = (current_value - prev_value) / prev_value if prev_value > 0 else 0\n",
        "\n",
        "        # Fee penalty (scaled)\n",
        "        fee_penalty = -fee_paid / self.initial_balance\n",
        "\n",
        "        # Risk adjustment based on volatility (optional)\n",
        "        if len(self.portfolio_history) > 10:\n",
        "            #returns = np.diff(self.portfolio_history[-10:]) / np.array(self.portfolio_history[-11:-1])\n",
        "            last_10 = np.array(self.portfolio_history[-10:])\n",
        "            prev_10 = np.array(self.portfolio_history[-11:-1])\n",
        "            returns = np.diff(last_10) / prev_10[:-1]\n",
        "            volatility = np.std(returns) if len(returns) > 1 else 0\n",
        "            # Sharpe-like adjustment (prefer consistent returns)\n",
        "            risk_adjustment = -volatility * 0.1 if volatility > 0 else 0\n",
        "        else:\n",
        "            risk_adjustment = 0\n",
        "\n",
        "        return raw_return + fee_penalty + risk_adjustment\n",
        "\n",
        "    def _get_portfolio_value(self, price_vec):\n",
        "        \"\"\"Calculate total portfolio value\"\"\"\n",
        "        return self.balance + np.sum(self.holdings * price_vec)\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render environment state\"\"\"\n",
        "        if self.render_mode == \"human\":\n",
        "            price_vec = self.df.loc[self.current_step, self.assets].values\n",
        "            portfolio_value = self._get_portfolio_value(price_vec)\n",
        "\n",
        "            print(f\"\\n--- Step: {self.current_step} ---\")\n",
        "            print(f\"Portfolio Value: ${portfolio_value:.2f}\")\n",
        "            print(f\"Return: {((portfolio_value - self.initial_balance) / self.initial_balance * 100):.2f}%\")\n",
        "            print(f\"Cash: ${self.balance:.2f}\")\n",
        "            print(f\"Current Asset: {self.current_asset}\")\n",
        "            print(f\"Holdings: {self.holdings}\")\n",
        "            print(f\"Prices: {dict(zip(self.assets, price_vec))}\")\n",
        "            print(f\"Episode Trades: {self.episode_trades}\")\n",
        "            print(f\"Episode Fees: ${self.episode_fees:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close environment\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "def mask_fn(env: gym.Env) -> np.ndarray:\n",
        "    \"\"\"Function to get action mask for ActionMasker wrapper\"\"\"\n",
        "    return env.action_masks()\n",
        "\n",
        "\n",
        "# Wrapper function to create masked environment\n",
        "def make_masked_env(df, **kwargs):\n",
        "    \"\"\"Create environment with action masking wrapper\"\"\"\n",
        "    def _init():\n",
        "        env = MultiAssetTradingEnv(df, **kwargs)\n",
        "        env = ActionMasker(env, mask_fn)\n",
        "        return env\n",
        "    return _init\n"
      ],
      "metadata": {
        "id": "0RkIIpu8LAfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98f91be-d555-4735-d6bb-157113d7eae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training utilities\n",
        "class TradingEnvTrainer:\n",
        "    \"\"\"Utility class for training trading agents with SB3\"\"\"\n",
        "\n",
        "    def __init__(self, df_train, df_val=None, env_kwargs=None):\n",
        "        self.df_train = df_train\n",
        "        self.df_val = df_val\n",
        "        self.env_kwargs = env_kwargs or {}\n",
        "\n",
        "    def create_env(self, df=None, n_envs=1):\n",
        "        \"\"\"Create training environment(s)\"\"\"\n",
        "        df = df if df is not None else self.df_train\n",
        "\n",
        "        if n_envs == 1:\n",
        "            return make_masked_env(df, **self.env_kwargs)()\n",
        "        else:\n",
        "            envs = [make_masked_env(df, **self.env_kwargs) for _ in range(n_envs)]\n",
        "            return DummyVecEnv(envs)\n",
        "\n",
        "    def train_agent(self, total_timesteps=100000, **ppo_kwargs):\n",
        "        \"\"\"Train MaskablePPO agent\"\"\"\n",
        "        # Create environment\n",
        "        env = self.create_env(n_envs=4)  # Use 4 parallel environments\n",
        "\n",
        "        # Default PPO configuration\n",
        "        default_config = {\n",
        "            \"learning_rate\": 3e-4,\n",
        "            \"n_steps\": 512,\n",
        "            \"batch_size\": 64,\n",
        "            \"n_epochs\": 10,\n",
        "            \"gamma\": 0.99,\n",
        "            \"gae_lambda\": 0.95,\n",
        "            \"clip_range\": 0.2,\n",
        "            \"ent_coef\": 0.01,\n",
        "            \"vf_coef\": 0.5,\n",
        "            \"max_grad_norm\": 0.5,\n",
        "            \"verbose\": 0,\n",
        "        }\n",
        "        default_config.update(ppo_kwargs)\n",
        "\n",
        "        # Create and train agent\n",
        "        model = MaskablePPO(\"MlpPolicy\", env, **default_config)\n",
        "        model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def evaluate_agent(self, model, df=None, n_episodes=10):\n",
        "        \"\"\"Evaluate trained agent\"\"\"\n",
        "        df = df if df is not None else self.df_val or self.df_train\n",
        "        env = self.create_env(df)\n",
        "\n",
        "        episode_returns = []\n",
        "        episode_trades = []\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            obs, info = env.reset()\n",
        "            done = False\n",
        "            episode_return = 0\n",
        "\n",
        "            while not done:\n",
        "                # Get action using the trained model\n",
        "                # Correct way to get predictions\n",
        "                action, _states = model.predict(obs, action_masks=env.action_masks(), deterministic=True)\n",
        "                obs, reward, terminated, truncated, info = env.step(action)\n",
        "                episode_return += reward\n",
        "                done = terminated or truncated\n",
        "\n",
        "            final_portfolio_value = info[\"portfolio_value\"]\n",
        "            final_return = info[\"return\"]\n",
        "            episode_returns.append(final_return)\n",
        "            episode_trades.append(info[\"episode_trades\"])\n",
        "\n",
        "            print(f\"Episode {episode + 1}: Return: {final_return:.2%}, \"\n",
        "                  f\"Value: ${final_portfolio_value:.2f}, Trades: {info['episode_trades']}\")\n",
        "\n",
        "        avg_return = np.mean(episode_returns)\n",
        "        avg_trades = np.mean(episode_trades)\n",
        "\n",
        "        print(f\"\\nAverage Return: {avg_return:.2%}\")\n",
        "        print(f\"Average Trades per Episode: {avg_trades:.1f}\")\n",
        "        print(f\"Return Std: {np.std(episode_returns):.2%}\")\n",
        "\n",
        "        return {\n",
        "            \"avg_return\": avg_return,\n",
        "            \"avg_trades\": avg_trades,\n",
        "            \"returns\": episode_returns,\n",
        "            \"trades\": episode_trades\n",
        "        }\n"
      ],
      "metadata": {
        "id": "v71QYV66RA_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Type, Union, Dict, Any\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from sb3_contrib import MaskablePPO\n",
        "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
        "\n",
        "class CNNTradingFeaturesExtractor(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    CNN-based feature extractor for trading time series data.\n",
        "    Treats the flattened observation as a 2D time series with multiple features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: spaces.Box, features_dim: int = 128, window_size = 30, n_features = 12):\n",
        "        # Calculate original 2D dimensions from flattened space\n",
        "        flat_dim = observation_space.shape[0]\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.n_features = n_features\n",
        "\n",
        "        print(f\"CNN Feature Extractor: window_size={self.window_size}, n_features={self.n_features}\")\n",
        "\n",
        "        super().__init__(observation_space, features_dim)\n",
        "\n",
        "        # 1D CNN for temporal patterns\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Reshape will be done in forward()\n",
        "            nn.Conv1d(self.n_features, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            #nn.LayerNorm1d(64),\n",
        "            nn.Conv1d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            #nn.LayerNorm1d(32),\n",
        "            nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
        "        )\n",
        "\n",
        "        # Final feature layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = observations.size(0)\n",
        "\n",
        "        # Reshape from flat to (batch, window_size, n_features)\n",
        "        obs_2d = observations.view(batch_size, self.window_size, self.n_features)\n",
        "\n",
        "        # Transpose for conv1d: (batch, n_features, window_size)\n",
        "        obs_conv = obs_2d.transpose(1, 2)\n",
        "\n",
        "        # Apply CNN\n",
        "        cnn_out = self.cnn(obs_conv)  # (batch, 32, 1)\n",
        "        cnn_flat = cnn_out.squeeze(-1)  # (batch, 32)\n",
        "\n",
        "        # Final features\n",
        "        features = self.fc(cnn_flat)\n",
        "        features = torch.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6) #especially needed if there is NormLayers\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "class LSTMTradingFeaturesExtractor(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    LSTM-based feature extractor for trading time series data.\n",
        "    Good for capturing long-term temporal dependencies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: spaces.Box, features_dim: int = 128, window_size = 30, n_features = 12):\n",
        "        # Calculate original 2D dimensions from flattened space\n",
        "        flat_dim = observation_space.shape[0]\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.n_features = n_features\n",
        "\n",
        "        print(f\"LSTM Feature Extractor: window_size={self.window_size}, n_features={self.n_features}\")\n",
        "\n",
        "        super().__init__(observation_space, features_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.n_features,\n",
        "            hidden_size=64,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # Final feature layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64, features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = observations.size(0)\n",
        "\n",
        "        # Reshape from flat to (batch, window_size, n_features)\n",
        "        obs_2d = observations.view(batch_size, self.window_size, self.n_features)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hidden, cell) = self.lstm(obs_2d)\n",
        "\n",
        "        # Use the last hidden state\n",
        "        last_hidden = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
        "\n",
        "        # Final features\n",
        "        features = self.fc(last_hidden)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "class AttentionTradingFeaturesExtractor(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    Attention-based feature extractor for trading data.\n",
        "    Uses multi-head attention to focus on important time steps and features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: spaces.Box, features_dim: int = 128, window_size = 30, n_features = 12):\n",
        "        flat_dim = observation_space.shape[0]\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.n_features = n_features\n",
        "\n",
        "        print(f\"Attention Feature Extractor: window_size={self.window_size}, n_features={self.n_features}\")\n",
        "\n",
        "        super().__init__(observation_space, features_dim)\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=self.n_features,\n",
        "            num_heads=3,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Layer norm\n",
        "        self.layer_norm = nn.LayerNorm(self.n_features)\n",
        "\n",
        "        # Final feature layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_features, features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = observations.size(0)\n",
        "\n",
        "        # Reshape from flat to (batch, window_size, n_features)\n",
        "        obs_2d = observations.view(batch_size, self.window_size, self.n_features)\n",
        "\n",
        "        # Self-attention\n",
        "        attn_out, _ = self.attention(obs_2d, obs_2d, obs_2d)\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        obs_attended = self.layer_norm(obs_2d + attn_out)\n",
        "\n",
        "        # Global average pooling over time dimension\n",
        "        pooled = torch.mean(obs_attended, dim=1)  # (batch, n_features)\n",
        "\n",
        "        # Final features\n",
        "        features = self.fc(pooled)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "class CustomTradingPolicy(MaskableActorCriticPolicy):\n",
        "    \"\"\"\n",
        "    Custom policy that uses our trading-specific feature extractors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Space,\n",
        "        action_space: spaces.Space,\n",
        "        lr_schedule,\n",
        "        net_arch: List[Union[int, Dict[str, List[int]]]] = None,\n",
        "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
        "        features_extractor_class: Type[BaseFeaturesExtractor] = CNNTradingFeaturesExtractor,\n",
        "        features_extractor_kwargs: Dict[str, Any] = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "\n",
        "        if net_arch is None:\n",
        "            net_arch = [dict(pi=[128, 128], vf=[128, 128])]\n",
        "\n",
        "        if features_extractor_kwargs is None:\n",
        "            features_extractor_kwargs = dict(features_dim=128)\n",
        "\n",
        "        super().__init__(\n",
        "            observation_space,\n",
        "            action_space,\n",
        "            lr_schedule,\n",
        "            net_arch=net_arch,\n",
        "            activation_fn=activation_fn,\n",
        "            features_extractor_class=features_extractor_class,\n",
        "            features_extractor_kwargs=features_extractor_kwargs,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "\n",
        "# Training configurations for different models\n",
        "def get_cnn_model_config():\n",
        "    \"\"\"Configuration for CNN-based trading model\"\"\"\n",
        "    return {\n",
        "        \"policy\": CustomTradingPolicy,\n",
        "        \"policy_kwargs\": {\n",
        "            \"features_extractor_class\": CNNTradingFeaturesExtractor,\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": 128},\n",
        "            \"net_arch\": [dict(pi=[128, 64], vf=[128, 64])],\n",
        "            \"activation_fn\": nn.ReLU,\n",
        "        },\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"n_steps\": 512,\n",
        "        \"batch_size\": 32,\n",
        "        \"n_epochs\": 5,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_range\": 0.2,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.5,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_lstm_model_config():\n",
        "    \"\"\"Configuration for LSTM-based trading model\"\"\"\n",
        "    return {\n",
        "        \"policy\": CustomTradingPolicy,\n",
        "        \"policy_kwargs\": {\n",
        "            \"features_extractor_class\": LSTMTradingFeaturesExtractor,\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": 128},\n",
        "            \"net_arch\": [dict(pi=[128, 64], vf=[128, 64])],\n",
        "            \"activation_fn\": nn.ReLU,\n",
        "        },\n",
        "        \"learning_rate\": 2e-4,  # Slightly lower for LSTM\n",
        "        \"n_steps\": 1024,        # Longer episodes for LSTM\n",
        "        \"batch_size\": 32,\n",
        "        \"n_epochs\": 5,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_range\": 0.2,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.5,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_attention_model_config():\n",
        "    \"\"\"Configuration for Attention-based trading model\"\"\"\n",
        "    return {\n",
        "        \"policy\": CustomTradingPolicy,\n",
        "        \"policy_kwargs\": {\n",
        "            \"features_extractor_class\": AttentionTradingFeaturesExtractor,\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": 128},\n",
        "            \"net_arch\": [dict(pi=[128, 64], vf=[128, 64])],\n",
        "            \"activation_fn\": nn.ReLU,\n",
        "        },\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"n_steps\": 512,\n",
        "        \"batch_size\": 32,\n",
        "        \"n_epochs\": 5,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_range\": 0.2,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.5,\n",
        "    }\n",
        "\n",
        "\n",
        "# Enhanced trainer with custom models\n",
        "class EnhancedTradingTrainer(TradingEnvTrainer):\n",
        "    \"\"\"Enhanced trainer with support for custom network architectures\"\"\"\n",
        "\n",
        "    def train_with_custom_model(self, model_type=\"cnn\", total_timesteps=100000):\n",
        "        \"\"\"Train with custom model architecture\"\"\"\n",
        "\n",
        "        # Get model configuration\n",
        "        if model_type.lower() == \"cnn\":\n",
        "            config = get_cnn_model_config()\n",
        "        elif model_type.lower() == \"lstm\":\n",
        "            config = get_lstm_model_config()\n",
        "        elif model_type.lower() == \"attention\":\n",
        "            config = get_attention_model_config()\n",
        "        else:\n",
        "            raise ValueError(\"model_type must be 'cnn', 'lstm', or 'attention'\")\n",
        "\n",
        "        # Create environment\n",
        "        env = self.create_env(n_envs=4) # 4\n",
        "\n",
        "        print(f\"Training with {model_type.upper()} model...\")\n",
        "\n",
        "        # Create and train model\n",
        "        model = MaskablePPO(env=env, **config)\n",
        "        model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "# Complete example usage\n",
        "def complete_training_example(df):\n",
        "    \"\"\"Complete training example with your data\"\"\"\n",
        "\n",
        "    # Split data\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    df_train = df.iloc[:split_idx]\n",
        "    df_val = df.iloc[split_idx:]\n",
        "\n",
        "    print(f\"Training data: {len(df_train)} samples\")\n",
        "    print(f\"Validation data: {len(df_val)} samples\")\n",
        "\n",
        "    # Create enhanced trainer\n",
        "    trainer = EnhancedTradingTrainer(\n",
        "        df_train,\n",
        "        df_val,\n",
        "        env_kwargs={\n",
        "            \"window_size\": 30,\n",
        "            \"initial_balance\": 10000,\n",
        "            \"fee_pct\": 0.001,\n",
        "            \"normalize_obs\": True\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Train different models and compare\n",
        "    models = {}\n",
        "    results = {}\n",
        "\n",
        "    for model_type in [\"cnn\", \"lstm\", \"attention\"]:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training {model_type.upper()} model\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        try:\n",
        "            # Train model\n",
        "            model = trainer.train_with_custom_model(\n",
        "                model_type=model_type,\n",
        "                total_timesteps=50000\n",
        "            )\n",
        "\n",
        "            # Save model\n",
        "            model.save(f\"trading_model_{model_type}\")\n",
        "            models[model_type] = model\n",
        "\n",
        "            # Evaluate\n",
        "            print(f\"\\nEvaluating {model_type.upper()} model...\")\n",
        "            eval_results = trainer.evaluate_agent(model, df_val, n_episodes=10)\n",
        "            results[model_type] = eval_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {model_type} model: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Compare results\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"MODEL COMPARISON\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for model_type, result in results.items():\n",
        "        print(f\"{model_type.upper()} Model:\")\n",
        "        print(f\"  Average Return: {result['avg_return']:.2%}\")\n",
        "        print(f\"  Return Std: {np.std(result['returns']):.2%}\")\n",
        "        print(f\"  Average Trades: {result['avg_trades']:.1f}\")\n",
        "        print(f\"  Best Return: {max(result['returns']):.2%}\")\n",
        "        print(f\"  Worst Return: {min(result['returns']):.2%}\")\n",
        "        print()\n",
        "\n",
        "    return models, results"
      ],
      "metadata": {
        "id": "hRntg-VAOblV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "\n",
        "def suppress_warnings():\n",
        "    \"\"\"\n",
        "    Suppress common deprecation warnings that don't affect functionality\n",
        "    \"\"\"\n",
        "    # Suppress specific datetime deprecation warning from Jupyter\n",
        "    warnings.filterwarnings(\"ignore\",\n",
        "                          message=\"datetime.datetime.utcnow\\\\(\\\\) is deprecated\",\n",
        "                          category=DeprecationWarning)\n",
        "\n",
        "    # Suppress other common warnings that clutter output\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "    # Suppress specific gym/gymnasium warnings\n",
        "    warnings.filterwarnings(\"ignore\",\n",
        "                          message=\".*Box bound precision lowered.*\",\n",
        "                          category=UserWarning)\n",
        "\n",
        "    # Suppress stable-baselines3 warnings\n",
        "    warnings.filterwarnings(\"ignore\",\n",
        "                          message=\".*Using `continuous_actions=False`.*\",\n",
        "                          category=UserWarning)\n",
        "\n",
        "\n",
        "def set_clean_environment():\n",
        "    \"\"\"\n",
        "    Set environment variables for cleaner output\n",
        "    \"\"\"\n",
        "\n",
        "    # Reduce other library verbosity\n",
        "    os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "\n",
        "suppress_warnings()\n",
        "set_clean_environment()"
      ],
      "metadata": {
        "id": "vL8Vyk4L9WWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Multi-Asset Trading Environment with Stable-Baselines3\n",
        "Works with DataFrame structure containing KOC_Close, XU100_Close, USDTRY_Close\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from sb3_contrib import MaskablePPO\n",
        "\n",
        "def quick_test_run(df):\n",
        "    \"\"\"Quick test to verify everything works\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"QUICK TEST RUN\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Use small subset for quick test\n",
        "    test_df = df.iloc[-200:] if len(df) > 200 else df\n",
        "    #print(test_df.head())\n",
        "\n",
        "    # Create environment\n",
        "    env = MultiAssetTradingEnv(\n",
        "        test_df,\n",
        "        window_size=20,\n",
        "        initial_balance=10000,\n",
        "        fee_pct=0.001,\n",
        "        normalize_obs=True,\n",
        "        render_mode=\"human\"\n",
        "    )\n",
        "\n",
        "    # Validate environment\n",
        "    check_env(env)\n",
        "    print(\"✓ Environment validation passed!\")\n",
        "\n",
        "    # Test a few random steps\n",
        "    obs, info = env.reset()\n",
        "    print(f\"✓ Environment reset successful. Observation shape: {obs.shape}\")\n",
        "\n",
        "    for i in range(5):\n",
        "        # Get valid actions\n",
        "        action_mask = env.action_masks()\n",
        "        valid_actions = np.where(action_mask)[0]\n",
        "        #print(action_mask, valid_actions)\n",
        "\n",
        "        # Take random valid action\n",
        "        action = np.random.choice(valid_actions)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        print(f\"Step {i+1}: Action={action}, Reward={reward:.4f}, \"\n",
        "              f\"Portfolio=${info['portfolio_value']:.2f}, Executed={info['executed']}\")\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    print(\"✓ Quick test completed successfully!\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def train_simple_model(df, timesteps=10000):\n",
        "    \"\"\"Train a simple baseline model\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING SIMPLE BASELINE MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Split data\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    df_train = df.iloc[:split_idx]\n",
        "    print(df_train.head())\n",
        "    df_val = df.iloc[split_idx:]\n",
        "\n",
        "    print(f\"Training data: {len(df_train)} samples\")\n",
        "    print(f\"Validation data: {len(df_val)} samples\")\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = TradingEnvTrainer(\n",
        "        df_train,\n",
        "        df_val,\n",
        "        env_kwargs={\n",
        "            \"window_size\": 30,\n",
        "            \"initial_balance\": 10000,\n",
        "            \"fee_pct\": 0.001,\n",
        "            \"normalize_obs\": True\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Train simple model\n",
        "    print(\"Training baseline model...\")\n",
        "    model = trainer.train_agent(\n",
        "        total_timesteps=timesteps,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=256,\n",
        "        batch_size=32,\n",
        "        n_epochs=5,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model.save(\"simple_trading_model\")\n",
        "    print(\"✓ Model saved as 'simple_trading_model'\")\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluating model...\")\n",
        "    results = trainer.evaluate_agent(model, df_val, n_episodes=5)\n",
        "\n",
        "    print(f\"\\nBaseline Results:\")\n",
        "    print(f\"Average Return: {results['avg_return']:.2%}\")\n",
        "    print(f\"Average Trades: {results['avg_trades']:.1f}\")\n",
        "\n",
        "    return model, results\n",
        "\n",
        "\n",
        "def train_advanced_models(df, timesteps=20000):\n",
        "    \"\"\"Train advanced models with custom architectures\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING ADVANCED MODELS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Split data\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    df_train = df.iloc[:split_idx]\n",
        "    df_val = df.iloc[split_idx:]\n",
        "\n",
        "    # Create enhanced trainer\n",
        "    trainer = EnhancedTradingTrainer(\n",
        "        df_train,\n",
        "        df_val,\n",
        "        env_kwargs={\n",
        "            \"window_size\": 30,\n",
        "            \"initial_balance\": 10000,\n",
        "            \"fee_pct\": 0.001,\n",
        "            \"normalize_obs\": True\n",
        "        }\n",
        "    )\n",
        "\n",
        "    models = {}\n",
        "    results = {}\n",
        "\n",
        "    # Train CNN model\n",
        "    try:\n",
        "        print(\"\\nTraining CNN model...\")\n",
        "        cnn_model = trainer.train_with_custom_model(\"cnn\", timesteps)\n",
        "        cnn_model.save(\"cnn_trading_model\")\n",
        "        models[\"cnn\"] = cnn_model\n",
        "\n",
        "        print(\"Evaluating CNN model...\")\n",
        "        results[\"cnn\"] = trainer.evaluate_agent(cnn_model, df_val, n_episodes=5)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"CNN training failed: {e}\")\n",
        "\n",
        "    # Train LSTM model\n",
        "    try:\n",
        "        print(\"\\nTraining LSTM model...\")\n",
        "        lstm_model = trainer.train_with_custom_model(\"lstm\", timesteps)\n",
        "        lstm_model.save(\"lstm_trading_model\")\n",
        "        models[\"lstm\"] = lstm_model\n",
        "\n",
        "        print(\"Evaluating LSTM model...\")\n",
        "        results[\"lstm\"] = trainer.evaluate_agent(lstm_model, df_val, n_episodes=5)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"LSTM training failed: {e}\")\n",
        "\n",
        "    # Train ATTENTION model\n",
        "    try:\n",
        "        print(\"\\nTraining ATTENTION model...\")\n",
        "        attn_model = trainer.train_with_custom_model(\"attention\", timesteps)\n",
        "        attn_model.save(\"attn_trading_model\")\n",
        "        models[\"attn\"] = attn_model\n",
        "\n",
        "        print(\"Evaluating ATTENTION model...\")\n",
        "        results[\"attn\"] = trainer.evaluate_agent(attn_model, df_val, n_episodes=5)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ATTENTION training failed: {e}\")\n",
        "\n",
        "    # Compare results\n",
        "    if results:\n",
        "        print(f\"\\n{'='*30}\")\n",
        "        print(\"ADVANCED MODEL COMPARISON\")\n",
        "        print(f\"{'='*30}\")\n",
        "\n",
        "        for model_type, result in results.items():\n",
        "            print(f\"{model_type.upper()}:\")\n",
        "            print(f\"  Avg Return: {result['avg_return']:.2%}\")\n",
        "            print(f\"  Avg Trades: {result['avg_trades']:.1f}\")\n",
        "            print()\n",
        "\n",
        "    return models, results\n",
        "\n",
        "\n",
        "\"\"\"Main training pipeline\"\"\"\n",
        "\n",
        "print(\"Multi-Asset Trading Agent Training Pipeline\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Quick validation test\n",
        "print(\"\\nValidate environment\")\n",
        "print(\"-\" * 30)\n",
        "quick_test_run(df)\n",
        "\n",
        "# Train simple baseline\n",
        "print(\"\\nTrain baseline model\")\n",
        "print(\"-\" * 30)\n",
        "simple_model, simple_results = train_simple_model(df, timesteps=100000)\n",
        "\n",
        "# Train advanced models (optional)\n",
        "print(\"\\nTrain advanced models\")\n",
        "print(\"-\" * 30)\n",
        "advanced_models, advanced_results = train_advanced_models(df, timesteps=200000)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"Models saved:\")\n",
        "\n",
        "print(\"\\nTo use a trained model:\")\n",
        "print(\"model = MaskablePPO.load('simple_trading_model')\")\n",
        "print(\"# Then use model.predict(obs, action_masks...) for trading decisions\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYnhNhRgM-hM",
        "outputId": "eaf85cc2-1508-4c86-87d3-869165898573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Asset Trading Agent Training Pipeline\n",
            "============================================================\n",
            "\n",
            "Validate environment\n",
            "------------------------------\n",
            "\n",
            "==================================================\n",
            "QUICK TEST RUN\n",
            "==================================================\n",
            "Warning: Invalid action 6 taken. Current asset: None\n",
            "Warning: Invalid action 1 taken. Current asset: 2\n",
            "Warning: Invalid action 3 taken. Current asset: 2\n",
            "Warning: Invalid action 5 taken. Current asset: 2\n",
            "Warning: Invalid action 2 taken. Current asset: 2\n",
            "Warning: Invalid action 5 taken. Current asset: 2\n",
            "Warning: Invalid action 1 taken. Current asset: 2\n",
            "Warning: Invalid action 1 taken. Current asset: 2\n",
            "Warning: Invalid action 4 taken. Current asset: 2\n",
            "✓ Environment validation passed!\n",
            "✓ Environment reset successful. Observation shape: (240,)\n",
            "Step 1: Action=1, Reward=-0.0058, Portfolio=$9951.81, Executed=buy_EREGL_AdjClose\n",
            "Step 2: Action=2, Reward=-0.0020, Portfolio=$9941.86, Executed=sell_EREGL_AdjClose\n",
            "Step 3: Action=5, Reward=0.0021, Portfolio=$9972.46, Executed=buy_KOC_AdjClose\n",
            "Step 4: Action=6, Reward=-0.0020, Portfolio=$9962.48, Executed=sell_KOC_AdjClose\n",
            "Step 5: Action=1, Reward=0.0018, Portfolio=$9990.71, Executed=buy_EREGL_AdjClose\n",
            "✓ Quick test completed successfully!\n",
            "\n",
            "Train baseline model\n",
            "------------------------------\n",
            "\n",
            "==================================================\n",
            "TRAINING SIMPLE BASELINE MODEL\n",
            "==================================================\n",
            "            EREGL_Volume  EREGL_AdjClose  SAHOL_Volume  SAHOL_AdjClose  \\\n",
            "2010-01-05      73491364        0.013520       5058266        2.207235   \n",
            "2010-01-06      27708427        0.013580       4992228        2.169179   \n",
            "2010-01-07      40402260        0.013342       2732948        2.169179   \n",
            "2010-01-08      46893994        0.013342      20476268        2.226263   \n",
            "2010-01-11      68805135        0.013520       8156529        2.169179   \n",
            "\n",
            "            KOC_Volume  KOC_AdjClose  XU100_Close  USDTRY_Close  \n",
            "2010-01-05    11165434      1.726188   541.148132        1.4727  \n",
            "2010-01-06     3897700      1.733472   545.471130        1.4714  \n",
            "2010-01-07     3662297      1.726188   549.726074        1.4715  \n",
            "2010-01-08     5993209      1.726188   547.976074        1.4580  \n",
            "2010-01-11     3878719      1.675204   539.208130        1.4502  \n",
            "Training data: 3216 samples\n",
            "Validation data: 804 samples\n",
            "Training baseline model...\n",
            "✓ Model saved as 'simple_trading_model'\n",
            "\n",
            "Evaluating model...\n",
            "Episode 1: Return: 484.80%, Value: $58479.72, Trades: 5\n",
            "Episode 2: Return: 484.80%, Value: $58479.72, Trades: 5\n",
            "Episode 3: Return: 484.80%, Value: $58479.72, Trades: 5\n",
            "Episode 4: Return: 484.80%, Value: $58479.72, Trades: 5\n",
            "Episode 5: Return: 484.80%, Value: $58479.72, Trades: 5\n",
            "\n",
            "Average Return: 484.80%\n",
            "Average Trades per Episode: 5.0\n",
            "Return Std: 0.00%\n",
            "\n",
            "Baseline Results:\n",
            "Average Return: 484.80%\n",
            "Average Trades: 5.0\n",
            "\n",
            "Train advanced models\n",
            "------------------------------\n",
            "\n",
            "==================================================\n",
            "TRAINING ADVANCED MODELS\n",
            "==================================================\n",
            "\n",
            "Training CNN model...\n",
            "Training with CNN model...\n",
            "CNN Feature Extractor: window_size=30, n_features=12\n",
            "Evaluating CNN model...\n",
            "Episode 1: Return: 529.02%, Value: $62902.44, Trades: 1\n",
            "Episode 2: Return: 529.02%, Value: $62902.44, Trades: 1\n",
            "Episode 3: Return: 529.02%, Value: $62902.44, Trades: 1\n",
            "Episode 4: Return: 529.02%, Value: $62902.44, Trades: 1\n",
            "Episode 5: Return: 529.02%, Value: $62902.44, Trades: 1\n",
            "\n",
            "Average Return: 529.02%\n",
            "Average Trades per Episode: 1.0\n",
            "Return Std: 0.00%\n",
            "\n",
            "Training LSTM model...\n",
            "Training with LSTM model...\n",
            "LSTM Feature Extractor: window_size=30, n_features=12\n"
          ]
        }
      ]
    }
  ]
}
{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13206350,"sourceType":"datasetVersion","datasetId":8370037},{"sourceId":264650060,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Accounting Fraud Detection using Normalizing-Flows**","metadata":{"id":"ZUprQ0hBuTGl"}},{"cell_type":"code","source":"!pip install nflows --quiet","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Cf6RZ7pumMu","outputId":"a641012a-a10e-41ce-c838-9f5a4b72349b","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T09:57:46.324407Z","iopub.execute_input":"2025-09-29T09:57:46.324669Z","iopub.status.idle":"2025-09-29T09:58:58.885481Z","shell.execute_reply.started":"2025-09-29T09:57:46.324651Z","shell.execute_reply":"2025-09-29T09:58:58.884810Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for nflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nimport os\n\n# nflows is a specialized library for normalizing flows.\nfrom nflows.flows.base import Flow\nfrom nflows.distributions.normal import StandardNormal\nfrom nflows.transforms.base import CompositeTransform\nfrom nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\nfrom nflows.transforms.permutations import RandomPermutation\nfrom nflows.transforms.normalization import BatchNorm, ActNorm\n\n# --- Data Simulation and Preprocessing (Unchanged) ---\ndef generate_synthetic_data(n_samples_normal=1000, n_samples_fraud=50):\n    \"\"\"Generates synthetic normal and fraudulent financial data.\"\"\"\n    mean_normal = np.array([0.5, 0.2, 0.7, 0.4, 0.6])\n    cov_normal = np.array([\n        [0.02, 0.01, 0.005, 0.005, 0.005],\n        [0.01, 0.03, 0.01, 0.005, 0.005],\n        [0.005, 0.01, 0.02, 0.005, 0.005],\n        [0.005, 0.005, 0.005, 0.03, 0.01],\n        [0.005, 0.005, 0.005, 0.01, 0.02]\n    ])\n    normal_data = np.random.multivariate_normal(mean_normal, cov_normal, n_samples_normal)\n\n    mean_fraud = np.array([0.1, 0.8, 0.2, 0.9, 0.1])\n    cov_fraud = np.array([\n        [0.05, -0.02, 0.01, -0.03, 0.01],\n        [-0.02, 0.06, -0.01, 0.04, -0.01],\n        [0.01, -0.01, 0.04, -0.02, 0.01],\n        [-0.03, 0.04, -0.02, 0.07, -0.02],\n        [0.01, -0.01, 0.01, -0.02, 0.05]\n    ])\n    fraud_data = np.random.multivariate_normal(mean_fraud, cov_fraud, n_samples_fraud)\n    return normal_data, fraud_data\n\n\"\"\"\nnormal_data, fraud_data = generate_synthetic_data()\nX_train = normal_data.astype(np.float32)\nX_test = np.vstack([normal_data[:200], fraud_data]).astype(np.float32)\ny_test = np.hstack([np.zeros(200), np.ones(fraud_data.shape[0])])\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\"\"\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"WT4xavUNuEbu","outputId":"91e92c8c-8d2f-490d-8a23-14ee9c00f796","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T11:01:55.034822Z","iopub.execute_input":"2025-09-29T11:01:55.035389Z","iopub.status.idle":"2025-09-29T11:01:55.044680Z","shell.execute_reply.started":"2025-09-29T11:01:55.035366Z","shell.execute_reply":"2025-09-29T11:01:55.043967Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'\\nnormal_data, fraud_data = generate_synthetic_data()\\nX_train = normal_data.astype(np.float32)\\nX_test = np.vstack([normal_data[:200], fraud_data]).astype(np.float32)\\ny_test = np.hstack([np.zeros(200), np.ones(fraud_data.shape[0])])\\n\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n'"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(r\"/kaggle/input/accounting-fraud/uscecchini28.csv\")\ndf.tail()\nprint(df.columns)\n\n# Check for infinite values in the entire dataframe\ninf_counts = df.isin([np.inf, -np.inf]).sum()\nprint(\"Columns with infinite values:\")\nprint(inf_counts[inf_counts > 0])\n\n# df.replace([np.inf, -np.inf], np.nan, inplace=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVyku_sIuuaW","outputId":"99960b74-d8af-4a1e-c875-1d2516a2e098","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:58:10.723094Z","iopub.execute_input":"2025-09-29T10:58:10.723776Z","iopub.status.idle":"2025-09-29T10:58:12.086733Z","shell.execute_reply.started":"2025-09-29T10:58:10.723750Z","shell.execute_reply":"2025-09-29T10:58:12.085912Z"}},"outputs":[{"name":"stdout","text":"Index(['fyear', 'gvkey', 'sich', 'insbnk', 'understatement', 'option',\n       'p_aaer', 'new_p_aaer', 'misstate', 'act', 'ap', 'at', 'ceq', 'che',\n       'cogs', 'csho', 'dlc', 'dltis', 'dltt', 'dp', 'ib', 'invt', 'ivao',\n       'ivst', 'lct', 'lt', 'ni', 'ppegt', 'pstk', 're', 'rect', 'sale',\n       'sstk', 'txp', 'txt', 'xint', 'prcc_f', 'dch_wc', 'ch_rsst', 'dch_rec',\n       'dch_inv', 'soft_assets', 'ch_cs', 'ch_cm', 'ch_roa', 'issue', 'bm',\n       'dpi', 'reoa', 'EBIT', 'ch_fcf'],\n      dtype='object')\nColumns with infinite values:\nSeries([], dtype: int64)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from scipy.stats import chisquare\nfrom sklearn.model_selection import train_test_split\n\n# --- raw variables and Benford's theoretical probabilities ---\nraw_variables = ['at', 'ap', 'ceq', 'che', 'csho', 'dltt', 'dp', 'ni', 'ppegt',\n                  'pstk', 're', 'rect', 'sale', 'lt', 'xint', 'ivao', 'dltis', 'sstk']\n\n# Theoretical probabilities for digits 1-9\nbenford_probs = np.array([np.log10(1 + 1 / d) for d in range(1, 10)])\n\n# --- function to calculate the Benford's Law Chi-squared statistic for a row ---\ndef calculate_benford_chi2(row):\n    \"\"\"Calculates the Chi-squared statistic for a given row based on Benford's Law.\"\"\"\n\n    # Extract first digits from the row's variables, excluding NaNs and zeros\n    first_digits = [\n        int(str(abs(x))[0]) for x in row[raw_variables]\n        if pd.notna(x) and x != 0\n    ]\n\n    if not first_digits:\n        return np.nan\n\n    # Count the occurrences of each digit\n    digit_counts = pd.Series(first_digits).value_counts().sort_index()\n\n    # Create an observed counts array for all 9 digits\n    observed = np.zeros(9)\n    for i, count in digit_counts.items():\n        if 1 <= i <= 9:\n            observed[i-1] = count\n\n    # Expected counts based on Benford's Law\n    n = len(first_digits)\n    expected = benford_probs * n\n\n    # Calculate Chi-squared statistic, handling zero values\n    expected_safe = expected + 1e-10\n    chi2_stat = np.sum((observed - expected)**2 / expected_safe)\n\n    return chi2_stat\n\n# --- Add the new feature column to the DataFrame ---\n# df['benford_chi2'] = df.apply(calculate_benford_chi2, axis=1)\n\n","metadata":{"id":"CfQi6LGLu66u","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:05:47.101274Z","iopub.execute_input":"2025-09-29T10:05:47.102005Z","iopub.status.idle":"2025-09-29T10:05:47.173899Z","shell.execute_reply.started":"2025-09-29T10:05:47.101978Z","shell.execute_reply":"2025-09-29T10:05:47.173336Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Separate all normal and fraud data\ndf_normal = df[df['misstate'] == 0]\ndf_fraud = df[df['misstate'] == 1]\n\n# Define features\nfeatures = ['act', 'ap', 'at', 'ceq', 'che',\n            'cogs', 'csho', 'dlc', 'dltis', 'dltt', 'dp', 'ib', 'invt', 'ivao',\n            'ivst', 'lct', 'lt', 'ni', 'ppegt', 'pstk', 're', 'rect', 'sale',\n            'sstk', 'txp', 'txt', 'xint', 'prcc_f', 'dch_wc', 'ch_rsst', 'dch_rec',\n            'dch_inv', 'soft_assets', 'ch_cs', 'ch_cm', 'ch_roa', 'issue', 'bm',\n            'dpi', 'reoa', 'ch_fcf']\n\n# Split the NORMAL data first into a training and a testing set\nX_normal = df_normal[features].dropna()\ny_normal = df_normal.loc[X_normal.index]['misstate']\n\nX_train, X_test_normal, y_train, y_test_normal = train_test_split(\n    X_normal, y_normal, test_size=0.05, random_state=42\n)\n\n# Now, fit the scaler ONLY on the training data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train) # Fit ONLY on X_train\n\nzero_variance_cols = X_train.columns[scaler.scale_ == 0]\nif len(zero_variance_cols) > 0:\n    print(\"Found columns with zero variance (standard deviation):\")\n    print(list(zero_variance_cols))\nelse:\n    print(\"No columns with zero variance found.\")\n\n# Prepare the fraud data\nX_test_fraud = df_fraud[features].dropna()\ny_test_fraud = df_fraud.loc[X_test_fraud.index]['misstate']\n\n# Transform the test sets using the FITTED scaler\nX_test_normal_scaled = scaler.transform(X_test_normal)\nX_test_fraud_scaled = scaler.transform(X_test_fraud)\n\n# Combine to create the final test set\nX_test_scaled = np.vstack([X_test_fraud_scaled, X_test_normal_scaled])\ny_test = pd.concat([y_test_fraud, y_test_normal])","metadata":{"id":"RmAJrI9uzvhl","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:58:19.393295Z","iopub.execute_input":"2025-09-29T10:58:19.393808Z","iopub.status.idle":"2025-09-29T10:58:19.600398Z","shell.execute_reply.started":"2025-09-29T10:58:19.393783Z","shell.execute_reply":"2025-09-29T10:58:19.599696Z"}},"outputs":[{"name":"stdout","text":"No columns with zero variance found.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(f\"Original training data shape: {X_train_scaled.shape}\")\n\n# Define a threshold for extreme values\nthreshold = 200\n\n# Find the indices of all rows that contain a value exceeding the threshold\noutlier_indices = np.where(np.abs(X_train_scaled) > threshold)[0]\n# Get a unique set of indices, as a row might have multiple outliers\noutlier_indices = np.unique(outlier_indices)\n\nprint(f\"Found {len(outlier_indices)} rows with extreme outlier values.\")\n\n# Remove the outlier rows\nX_train_scaled = np.delete(X_train_scaled, outlier_indices, axis=0)\n\nprint(f\"New training data shape after filtering: {X_train_scaled.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYmbSvDKzR9L","outputId":"7755bf33-ce3d-4fd1-b842-1009b54c31f0","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:58:24.641865Z","iopub.execute_input":"2025-09-29T10:58:24.642359Z","iopub.status.idle":"2025-09-29T10:58:24.702551Z","shell.execute_reply.started":"2025-09-29T10:58:24.642337Z","shell.execute_reply":"2025-09-29T10:58:24.701955Z"}},"outputs":[{"name":"stdout","text":"Original training data shape: (119295, 41)\nFound 0 rows with extreme outlier values.\nNew training data shape after filtering: (119295, 41)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Convert to PyTorch Tensors\nX_train_tensor = torch.from_numpy(X_train_scaled.astype(np.float64))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float64))\nX_test_tensor = torch.from_numpy(X_test_scaled.astype(np.float64))\n\n# Check for NaNs and infinite values in tensors\nif torch.isnan(X_train_tensor).any() or torch.isinf(X_train_tensor).any():\n    print(\"NaN or infinite values found in X_train_tensor!\")\nif torch.isnan(y_train_tensor).any() or torch.isinf(y_train_tensor).any():\n    print(\"NaN or infinite values found in y_train_tensor!\")\nif torch.isnan(X_test_tensor).any() or torch.isinf(X_test_tensor).any():\n    print(\"NaN or infinite values found in X_test_tensor!\")\n\n# Create datasets from the scaled tensors\ntrain_dataset = TensorDataset(X_train_tensor)\n\n# Create a validation split\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ndef weights_init(m):\n    \"\"\"\n    Applies a custom weight initialization to the model's layers.\n    Initializes linear layers with a normal distribution (mean=0, std=0.02)\n    and sets biases to zero.\n    \"\"\"\n    if isinstance(m, torch.nn.Linear):\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n# --- Define the Flow-based Model using nflows ---\ndef create_nflows_model(input_dim, num_layers=6, hidden_dim=256):\n    \"\"\"Creates a RealNVP-style model using the nflows library.\"\"\"\n    base_dist = StandardNormal(shape=[input_dim])\n\n    transforms = []\n    for _ in range(num_layers):\n        transforms.append(RandomPermutation(features=input_dim))\n        transforms.append(MaskedAffineAutoregressiveTransform(\n            features=input_dim,\n            hidden_features=hidden_dim\n        ))\n        transforms.append(ActNorm(features=input_dim))\n\n    transform = CompositeTransform(transforms)\n\n    flow = Flow(transform, base_dist)\n    return flow","metadata":{"id":"ptzCE-UWPrOI","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T11:02:21.585874Z","iopub.execute_input":"2025-09-29T11:02:21.586579Z","iopub.status.idle":"2025-09-29T11:02:21.633086Z","shell.execute_reply.started":"2025-09-29T11:02:21.586556Z","shell.execute_reply":"2025-09-29T11:02:21.632515Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# --- Forensic Debugging Cell ---\nprint(\"--- Starting Forensic Debugging ---\")\ntrain_dataset = TensorDataset(X_train_tensor)\n\n# Use the exact same (simplified) settings that are failing\ninput_dim = X_train_scaled.shape[1]\nnum_coupling_layers = 8\nhidden_dim = 256\nbatch_size = 256 # Ensure this matches your training batch_size\n\n# Re-create the DataLoader to ensure we get a consistent first batch\n# NOTE: Set shuffle=False to make the test repeatable.\ntrain_loader_debug = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\n# Get the VERY FIRST batch of data\ntry:\n    first_batch = next(iter(train_loader_debug))[0]\n    print(f\"\\nSuccessfully loaded one batch of size: {first_batch.shape}\")\nexcept StopIteration:\n    print(\"Error: DataLoader is empty. Check your train_dataset.\")\n    # Stop here if there's no data\n\n# Inspect the batch itself for any issues post-scaling and loading\nprint(f\"Batch Stats: Min={first_batch.min():.4f}, Max={first_batch.max():.4f}, Mean={first_batch.mean():.4f}\")\nif torch.isnan(first_batch).any() or torch.isinf(first_batch).any():\n    print(\"!!! FATAL ERROR: The batch itself contains NaNs or Infs before entering the model!\")\nelse:\n    print(\"✅ Batch data appears clean (no NaNs or Infs).\")\n\n# Create a fresh model instance and run a single forward pass\nmodel_debug = create_nflows_model(input_dim, num_coupling_layers, hidden_dim)\nmodel_debug.double()\nmodel_debug.eval() # Set to evaluation mode for a clean forward pass\n\nprint(\"\\n--- Running Forward Pass on Single Batch ---\")\nwith torch.no_grad():\n    try:\n        log_probs = model_debug.log_prob(inputs=first_batch)\n\n        # 6. Check for issues in the model's output\n        nan_mask = torch.isnan(log_probs)\n        inf_mask = torch.isinf(log_probs)\n\n        if nan_mask.any():\n            print(f\"🔴 FAILURE: Found {nan_mask.sum()} NaN(s) in the log_prob output!\")\n            # Find the specific row(s) in the batch that caused the NaN\n            problem_indices = nan_mask.nonzero(as_tuple=True)[0]\n            print(f\"Data row at index {problem_indices[0]} in the batch caused the NaN.\")\n            print(\"\\n--- Problematic Data Point ---\")\n            print(first_batch[problem_indices[0]])\n            print(\"------------------------------\")\n\n        elif inf_mask.any():\n            print(f\"🔴 FAILURE: Found {inf_mask.sum()} Inf(s) in the log_prob output!\")\n            problem_indices = inf_mask.nonzero(as_tuple=True)[0]\n            print(f\"Data row at index {problem_indices[0]} in the batch caused the Inf.\")\n            print(\"\\n--- Problematic Data Point ---\")\n            print(first_batch[problem_indices[0]])\n            print(\"------------------------------\")\n        else:\n            print(f\"✅ SUCCESS: The forward pass is clean.\")\n            print(f\"Log Probs Stats: Min={log_probs.min():.4f}, Max={log_probs.max():.4f}\")\n            print(\"This suggests the NaN may be occurring in the backward pass (gradient calculation).\")\n\n\n    except Exception as e:\n        print(f\"\\nAn exception occurred during the forward pass: {e}\")\n\n# Clean up to avoid interfering with the main training loop\ndel model_debug, train_loader_debug","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlG0yQdBxyGX","outputId":"d13d4d78-c091-4668-e60f-f79a274cd2c5","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T11:02:35.037521Z","iopub.execute_input":"2025-09-29T11:02:35.038065Z","iopub.status.idle":"2025-09-29T11:02:35.126266Z","shell.execute_reply.started":"2025-09-29T11:02:35.038039Z","shell.execute_reply":"2025-09-29T11:02:35.125421Z"}},"outputs":[{"name":"stdout","text":"--- Starting Forensic Debugging ---\n\nSuccessfully loaded one batch of size: torch.Size([256, 41])\nBatch Stats: Min=-6.2611, Max=35.7735, Mean=-0.0104\n✅ Batch data appears clean (no NaNs or Infs).\n\n--- Running Forward Pass on Single Batch ---\n✅ SUCCESS: The forward pass is clean.\nLog Probs Stats: Min=-190.3461, Max=-155.3461\nThis suggests the NaN may be occurring in the backward pass (gradient calculation).\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import torch.nn as nn\n\n# --- Training the Model ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ninput_dim = X_train_scaled.shape[1]\nnum_coupling_layers = 15\nhidden_dim = 512\nlearning_rate = 1e-5\nbatch_size = 256\nepochs = 100\npatience = 10 # For early stopping\nbest_model_path = \"best_model.pth\"\nweight_decay = 1e-5\n\nif torch.isnan(torch.tensor(X_train_scaled)).any():\n    raise ValueError(\"Input data contains NaN values. Please clean the data before training.\")\nelse:\n    print(\"Data integrity check passed: No NaN values found in input data.\")\n\nmodel = create_nflows_model(input_dim, num_coupling_layers, hidden_dim)\nmodel.double()\nmodel.apply(weights_init)\n\nif torch.cuda.device_count() > 1:\n    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\nmodel.to(device)\nprint(f\"Using device: {device}\")\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Create DataLoaders for train and validation sets\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\nprint(\"Starting model training...\")\n\nbest_val_loss = float('inf')\nearly_stopping_counter = 0\n\nfor epoch in range(epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    for (batch,) in train_loader:  \n        batch = batch.to(device)\n        optimizer.zero_grad()\n        if isinstance(model, nn.DataParallel):\n            loss = -model.module.log_prob(inputs=batch).mean()\n        else:\n            loss = -model.log_prob(inputs=batch).mean()\n        \n        if torch.isnan(loss):\n            print(\"NaN loss detected. Stopping training.\")\n            break\n            \n        loss.backward()\n\n        # gradient clipping for stabilization\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n\n        optimizer.step()\n        train_loss += loss.item()\n    \n    if torch.isnan(loss): # Break outer loop if NaN was detected\n        break\n        \n    avg_train_loss = train_loss / len(train_loader)\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for (batch,) in val_loader:\n            batch = batch.to(device)\n            if isinstance(model, nn.DataParallel):\n                loss = -model.module.log_prob(inputs=batch).mean()\n            else:\n                loss = -model.log_prob(inputs=batch).mean()\n            val_loss += loss.item()\n    avg_val_loss = val_loss / len(val_loader)\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n    if avg_val_loss is None or torch.isnan(torch.tensor(avg_val_loss)):\n        print(f\"Validation loss is None at epoch {epoch + 1}. Stopping training.\")\n        break\n\n    # Early stopping and best model saving\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        state_to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n        torch.save(state_to_save, best_model_path)\n        early_stopping_counter = 0\n        print(f\"  -> New best model saved with val_loss: {best_val_loss:.4f}\")\n    else:\n        early_stopping_counter += 1\n\n    if early_stopping_counter >= patience:\n        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n        break\n\nprint(\"\\nTraining finished.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnCcQQd9uE4V","outputId":"3d9ce530-6002-4912-d458-22fc0af84e42","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T11:03:30.020195Z","iopub.execute_input":"2025-09-29T11:03:30.020491Z","iopub.status.idle":"2025-09-29T13:25:01.167906Z","shell.execute_reply.started":"2025-09-29T11:03:30.020466Z","shell.execute_reply":"2025-09-29T13:25:01.167221Z"}},"outputs":[{"name":"stdout","text":"Data integrity check passed: No NaN values found in input data.\nLet's use 2 GPUs!\nUsing device: cuda\nStarting model training...\n  -> New best model saved with val_loss: 26.8816\n  -> New best model saved with val_loss: -16.1035\n  -> New best model saved with val_loss: -24.1473\n  -> New best model saved with val_loss: -28.8335\n  -> New best model saved with val_loss: -32.3947\n  -> New best model saved with val_loss: -35.2613\n  -> New best model saved with val_loss: -37.7187\n  -> New best model saved with val_loss: -39.5027\n  -> New best model saved with val_loss: -41.1665\nEpoch 10, Train Loss: -41.9849, Val Loss: -42.8530\n  -> New best model saved with val_loss: -42.8530\n  -> New best model saved with val_loss: -44.2922\n  -> New best model saved with val_loss: -45.7391\n  -> New best model saved with val_loss: -46.6929\n  -> New best model saved with val_loss: -47.5988\n  -> New best model saved with val_loss: -49.0684\n  -> New best model saved with val_loss: -49.8541\n  -> New best model saved with val_loss: -50.9429\n  -> New best model saved with val_loss: -51.7257\n  -> New best model saved with val_loss: -52.9450\nEpoch 20, Train Loss: -53.1792, Val Loss: -53.7276\n  -> New best model saved with val_loss: -53.7276\n  -> New best model saved with val_loss: -54.5294\n  -> New best model saved with val_loss: -55.5772\n  -> New best model saved with val_loss: -57.1129\n  -> New best model saved with val_loss: -58.6685\n  -> New best model saved with val_loss: -59.1047\n  -> New best model saved with val_loss: -60.1448\n  -> New best model saved with val_loss: -60.4104\n  -> New best model saved with val_loss: -61.2708\nEpoch 30, Train Loss: -62.0105, Val Loss: -62.5786\n  -> New best model saved with val_loss: -62.5786\n  -> New best model saved with val_loss: -63.6723\n  -> New best model saved with val_loss: -63.8196\n  -> New best model saved with val_loss: -65.4441\n  -> New best model saved with val_loss: -65.5842\n  -> New best model saved with val_loss: -66.3441\n  -> New best model saved with val_loss: -66.5768\nEpoch 40, Train Loss: -67.3137, Val Loss: -67.7599\n  -> New best model saved with val_loss: -67.7599\n  -> New best model saved with val_loss: -68.2773\n  -> New best model saved with val_loss: -68.8688\n  -> New best model saved with val_loss: -70.0467\nEpoch 50, Train Loss: -69.9469, Val Loss: -70.0812\n  -> New best model saved with val_loss: -70.0812\n  -> New best model saved with val_loss: -70.3138\n  -> New best model saved with val_loss: -70.4898\n  -> New best model saved with val_loss: -70.9498\n  -> New best model saved with val_loss: -71.0747\n  -> New best model saved with val_loss: -71.8349\n  -> New best model saved with val_loss: -72.2617\nEpoch 60, Train Loss: -71.3033, Val Loss: -71.6451\n  -> New best model saved with val_loss: -72.6741\n  -> New best model saved with val_loss: -72.8231\n  -> New best model saved with val_loss: -73.2384\nEpoch 70, Train Loss: -72.5437, Val Loss: -73.2412\n  -> New best model saved with val_loss: -73.2412\n  -> New best model saved with val_loss: -73.3350\n  -> New best model saved with val_loss: -73.3670\n  -> New best model saved with val_loss: -73.5093\nEpoch 80, Train Loss: -73.5979, Val Loss: -74.1787\n  -> New best model saved with val_loss: -74.1787\n  -> New best model saved with val_loss: -74.4946\nEpoch 90, Train Loss: -74.2245, Val Loss: -74.5050\n  -> New best model saved with val_loss: -74.5050\n  -> New best model saved with val_loss: -74.7247\nEpoch 100, Train Loss: -74.5439, Val Loss: -73.3088\n\nTraining finished.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_default_dtype(torch.float64)\n\nmodel = create_nflows_model(input_dim, num_coupling_layers, hidden_dim)\nmodel.double()\n\nif os.path.exists(best_model_path):\n    print(f\"\\nLoading best model from '{best_model_path}' for evaluation.\")\n    state_dict = torch.load(best_model_path, map_location=device)\n\n    # Handle the 'module.' prefix if the model was saved using DataParallel\n    if next(iter(state_dict)).startswith('module.'):\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:] # remove `module.`\n            new_state_dict[name] = v\n        model.load_state_dict(new_state_dict)\n    else:\n        model.load_state_dict(state_dict)\nelse:\n    print(\"\\nWarning: No saved model found. Cannot perform evaluation.\")\n    exit()\n\nmodel.to(device)\n\n# Wrap with DataParallel if using multiple GPUs\nif device.type == 'cuda' and torch.cuda.device_count() > 1:\n    print(\"Wrapping loaded model with DataParallel for evaluation.\")\n    model = torch.nn.DataParallel(model)\n\nmodel.eval()\n\n\ndef detect_anomalies(model, data_loader, threshold_percentile=5):\n    \"\"\"Calculates log probabilities and flags anomalies.\"\"\"\n    model.eval()\n    log_probs_list = []\n    \n    # Determine if the model is wrapped with DataParallel\n    is_parallel = isinstance(model, nn.DataParallel)\n    \n    with torch.no_grad():\n        for (batch,) in data_loader:\n            # Move data to the correct device\n            batch = batch.to(device)\n            \n            if is_parallel:\n                log_probs = model.module.log_prob(inputs=batch)\n            else:\n                log_probs = model.log_prob(inputs=batch)\n            \n            # Move results back to CPU for concatenation\n            log_probs_list.append(log_probs.cpu())\n            \n    log_probs_tensor = torch.cat(log_probs_list)\n\n    # We need the full original training data to set a robust threshold\n    full_train_tensor = torch.from_numpy(X_train_scaled.astype(np.float64)).to(device)\n    \n    with torch.no_grad():\n        if is_parallel:\n            train_log_probs = model.module.log_prob(inputs=full_train_tensor)\n        else:\n            train_log_probs = model.log_prob(inputs=full_train_tensor)\n\n    # Move to CPU for numpy operations\n    threshold = np.percentile(train_log_probs.cpu().numpy(), threshold_percentile)\n    print(f\"\\nAnomaly threshold (log probability): {threshold:.4f}\")\n\n    predictions = log_probs_tensor.numpy() < threshold\n    return predictions, log_probs_tensor.numpy()\n\n\nX_test_tensor = torch.from_numpy(X_test_scaled.astype(np.float64))\ntest_dataset = TensorDataset(X_test_tensor)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nanomaly_predictions, log_probabilities = detect_anomalies(model, test_loader)\n\ncorrect_predictions = (anomaly_predictions == y_test)\naccuracy = np.mean(correct_predictions)\ntrue_positives = np.sum(anomaly_predictions & (y_test == 1))\nfalse_positives = np.sum(anomaly_predictions & (y_test == 0))\nrecall = true_positives / np.sum(y_test == 1) if np.sum(y_test == 1) > 0 else 0\n\nprint(\"\\n--- Performance Metrics ---\")\nprint(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Fraud cases detected (Recall): {recall * 100:.2f}% ({true_positives}/{np.sum(y_test==1)})\")\nprint(f\"Normal cases flagged as fraud (False Positives): {false_positives}/{np.sum(y_test==0)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:35:49.787782Z","iopub.execute_input":"2025-09-29T13:35:49.788257Z","iopub.status.idle":"2025-09-29T13:36:09.591936Z","shell.execute_reply.started":"2025-09-29T13:35:49.788236Z","shell.execute_reply":"2025-09-29T13:36:09.591230Z"}},"outputs":[{"name":"stdout","text":"\nLoading best model from 'best_model.pth' for evaluation.\nWrapping loaded model with DataParallel for evaluation.\n\nAnomaly threshold (log probability): -23.8393\n\n--- Performance Metrics ---\nOverall Accuracy: 84.46%\nFraud cases detected (Recall): 12.65% (115/909)\nNormal cases flagged as fraud (False Positives): 323/6279\n","output_type":"stream"}],"execution_count":45}]}
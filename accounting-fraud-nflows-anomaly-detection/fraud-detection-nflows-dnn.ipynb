{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13206350,"sourceType":"datasetVersion","datasetId":8370037},{"sourceId":264650060,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Accounting Fraud Detection using Normalizing-Flows**","metadata":{"id":"ZUprQ0hBuTGl"}},{"cell_type":"code","source":"!pip install --upgrade nflows matplotlib seaborn --quiet\n#!pip install pykan\n!pip install git+https://github.com/Blealtan/efficient-kan.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Cf6RZ7pumMu","outputId":"a641012a-a10e-41ce-c838-9f5a4b72349b","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:03:20.685861Z","iopub.execute_input":"2025-10-12T16:03:20.686015Z","iopub.status.idle":"2025-10-12T16:04:49.452750Z","shell.execute_reply.started":"2025-10-12T16:03:20.686001Z","shell.execute_reply":"2025-10-12T16:04:49.452006Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for nflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting git+https://github.com/Blealtan/efficient-kan.git\n  Cloning https://github.com/Blealtan/efficient-kan.git to /tmp/pip-req-build-p4ukupz4\n  Running command git clone --filter=blob:none --quiet https://github.com/Blealtan/efficient-kan.git /tmp/pip-req-build-p4ukupz4\n  Resolved https://github.com/Blealtan/efficient-kan.git to commit 7b6ce1c87f18c8bc90c208f6b494042344216b11\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from efficient-kan==0.1.0) (2.6.0+cu124)\nRequirement already satisfied: pytest>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from efficient-kan==0.1.0) (8.3.5)\nRequirement already satisfied: tqdm>=4.66.2 in /usr/local/lib/python3.11/dist-packages (from efficient-kan==0.1.0) (4.67.1)\nRequirement already satisfied: torchvision>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from efficient-kan==0.1.0) (0.21.0+cu124)\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=8.2.0->efficient-kan==0.1.0) (2.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest>=8.2.0->efficient-kan==0.1.0) (25.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=8.2.0->efficient-kan==0.1.0) (1.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->efficient-kan==0.1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->efficient-kan==0.1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.18.0->efficient-kan==0.1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.18.0->efficient-kan==0.1.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->efficient-kan==0.1.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision>=0.18.0->efficient-kan==0.1.0) (2024.2.0)\nBuilding wheels for collected packages: efficient-kan\n  Building wheel for efficient-kan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficient-kan: filename=efficient_kan-0.1.0-py3-none-any.whl size=5983 sha256=d95afdc053ebfe08ca7808ee383330c6e905f9028a9da2f2c2782e4ed655e8fd\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5ncnbuye/wheels/da/a3/b4/30b3e3df2edaded3c87227cb9faea5878f18ec03e1c2137839\nSuccessfully built efficient-kan\nInstalling collected packages: efficient-kan\nSuccessfully installed efficient-kan-0.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom nflows.flows.base import Flow\nfrom nflows.distributions.normal import StandardNormal\nfrom nflows.transforms.base import CompositeTransform\nfrom nflows.transforms.autoregressive import MaskedPiecewiseRationalQuadraticAutoregressiveTransform\nfrom nflows.transforms.permutations import RandomPermutation\nfrom nflows.transforms import (\n    LULinear,\n    PiecewiseRationalQuadraticCouplingTransform,\n    AffineCouplingTransform\n)\nfrom nflows.transforms.normalization import ActNorm\nfrom nflows.utils import create_alternating_binary_mask\nfrom efficient_kan import KAN as EfficientKAN\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_synthetic_data(n_samples_normal=1000, n_samples_fraud=50):\n    \"\"\"Generates synthetic normal and fraudulent financial data.\"\"\"\n    mean_normal = np.array([0.5, 0.2, 0.7, 0.4, 0.6])\n    cov_normal = np.array([\n        [0.02, 0.01, 0.005, 0.005, 0.005],\n        [0.01, 0.03, 0.01, 0.005, 0.005],\n        [0.005, 0.01, 0.02, 0.005, 0.005],\n        [0.005, 0.005, 0.005, 0.03, 0.01],\n        [0.005, 0.005, 0.005, 0.01, 0.02]\n    ])\n    normal_data = np.random.multivariate_normal(mean_normal, cov_normal, n_samples_normal)\n\n    mean_fraud = np.array([0.1, 0.8, 0.2, 0.9, 0.1])\n    cov_fraud = np.array([\n        [0.05, -0.02, 0.01, -0.03, 0.01],\n        [-0.02, 0.06, -0.01, 0.04, -0.01],\n        [0.01, -0.01, 0.04, -0.02, 0.01],\n        [-0.03, 0.04, -0.02, 0.07, -0.02],\n        [0.01, -0.01, 0.01, -0.02, 0.05]\n    ])\n    fraud_data = np.random.multivariate_normal(mean_fraud, cov_fraud, n_samples_fraud)\n    return normal_data, fraud_data\n\n\"\"\"\nnormal_data, fraud_data = generate_synthetic_data()\nX_train = normal_data.astype(np.float32)\nX_test = np.vstack([normal_data[:200], fraud_data]).astype(np.float32)\ny_test = np.hstack([np.zeros(200), np.ones(fraud_data.shape[0])])\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\"\"\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"WT4xavUNuEbu","outputId":"91e92c8c-8d2f-490d-8a23-14ee9c00f796","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:05:32.547361Z","iopub.execute_input":"2025-10-12T16:05:32.547789Z","iopub.status.idle":"2025-10-12T16:05:36.280835Z","shell.execute_reply.started":"2025-10-12T16:05:32.547758Z","shell.execute_reply":"2025-10-12T16:05:36.280094Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\nnormal_data, fraud_data = generate_synthetic_data()\\nX_train = normal_data.astype(np.float32)\\nX_test = np.vstack([normal_data[:200], fraud_data]).astype(np.float32)\\ny_test = np.hstack([np.zeros(200), np.ones(fraud_data.shape[0])])\\n\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(r\"/kaggle/input/accounting-fraud/uscecchini28.csv\")\nprint(df.head())\nprint(\"\\nOriginal Skewness of Features:\")\nprint(df.skew())\n\n# Check for infinite values in the entire dataframe\ninf_counts = df.isin([np.inf, -np.inf]).sum()\nprint(\"Columns with infinite values:\")\nprint(inf_counts[inf_counts > 0])\n\n# df.replace([np.inf, -np.inf], np.nan, inplace=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVyku_sIuuaW","outputId":"99960b74-d8af-4a1e-c875-1d2516a2e098","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:18:25.128827Z","iopub.execute_input":"2025-10-12T18:18:25.129119Z","iopub.status.idle":"2025-10-12T18:18:26.692530Z","shell.execute_reply.started":"2025-10-12T18:18:25.129099Z","shell.execute_reply":"2025-10-12T18:18:26.691777Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"name":"stdout","text":"   fyear  gvkey    sich  insbnk  understatement  option  p_aaer  new_p_aaer  \\\n0   1990   1009  3460.0       0               0       0     NaN         NaN   \n1   1990   1011  4841.0       0               0       0     NaN         NaN   \n2   1990   1017  3812.0       0               0       0     NaN         NaN   \n3   1990   1021  3861.0       0               0       0     NaN         NaN   \n4   1990   1028  7385.0       0               0       0     NaN         NaN   \n\n   misstate     act  ...  soft_assets     ch_cs     ch_cm    ch_roa  issue  \\\n0         0  10.047  ...     0.312448  0.095082  0.082631 -0.019761      1   \n1         0   1.247  ...     0.315904  0.188832 -0.211389 -0.117832      1   \n2         0  55.040  ...     0.605342  0.097551 -0.105780  0.091206      1   \n3         0  24.684  ...     0.793068 -0.005725 -0.249704  0.017545      1   \n4         0  17.325  ...     0.869182 -0.231536 -1.674893 -0.466667      0   \n\n         bm       dpi      reoa      EBIT    ch_fcf  \n0  0.413170  0.873555  0.167620  0.161961 -0.042140  \n1  0.157887  0.745139 -0.428957 -0.157888  0.100228  \n2  2.231337  1.015131  0.394768  0.063681  0.066348  \n3  1.043582  1.026261  0.094822  0.088347 -0.017358  \n4 -1.602508  0.598443 -0.942379 -0.700821  0.130349  \n\n[5 rows x 51 columns]\n\nOriginal Skewness of Features:\nfyear              0.052576\ngvkey              1.245898\nsich               0.495062\ninsbnk             5.541949\nunderstatement    74.928172\noption            33.734268\np_aaer            -0.365102\nnew_p_aaer        -0.365102\nmisstate          12.186413\nact               14.622570\nap                15.746885\nat                13.646182\nceq               18.430113\nche               20.863528\ncogs              21.973491\ncsho              87.078875\ndlc               29.182886\ndltis             52.000890\ndltt              13.879783\ndp                16.598885\nib                22.205472\ninvt              18.280336\nivao              59.297631\nivst              42.505499\nlct               15.493710\nlt                13.058111\nni                17.413871\nppegt             17.449189\npstk              35.790146\nre                25.850183\nrect              19.949100\nsale              19.307460\nsstk              58.141274\ntxp               25.417456\ntxt               22.587052\nxint              13.185318\nprcc_f            50.949417\ndch_wc            -2.045425\nch_rsst           -1.162339\ndch_rec            0.488225\ndch_inv            0.623405\nsoft_assets       -0.147511\nch_cs              2.863447\nch_cm             -0.327478\nch_roa            -0.132948\nissue             -2.117430\nbm                -2.403488\ndpi                3.455301\nreoa              -5.572921\nEBIT              -4.992937\nch_fcf             1.988151\ndtype: float64\nColumns with infinite values:\nSeries([], dtype: int64)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from scipy.stats import chisquare\nfrom sklearn.model_selection import train_test_split\n\n# --- raw variables and Benford's theoretical probabilities ---\nraw_variables = ['at', 'ap', 'ceq', 'che', 'csho', 'dltt', 'dp', 'ni', 'ppegt',\n                  'pstk', 're', 'rect', 'sale', 'lt', 'xint', 'ivao', 'dltis', 'sstk']\n\n# Theoretical probabilities for digits 1-9\nbenford_probs = np.array([np.log10(1 + 1 / d) for d in range(1, 10)])\n\n# --- function to calculate the Benford's Law Chi-squared statistic for a row ---\ndef calculate_benford_chi2(row):\n    \"\"\"Calculates the Chi-squared statistic for a given row based on Benford's Law.\"\"\"\n\n    # Extract first digits from the row's variables, excluding NaNs and zeros\n    first_digits = [\n        int(str(abs(x))[0]) for x in row[raw_variables]\n        if pd.notna(x) and x != 0\n    ]\n\n    if not first_digits:\n        return np.nan\n\n    # Count the occurrences of each digit\n    digit_counts = pd.Series(first_digits).value_counts().sort_index()\n\n    # Create an observed counts array for all 9 digits\n    observed = np.zeros(9)\n    for i, count in digit_counts.items():\n        if 1 <= i <= 9:\n            observed[i-1] = count\n\n    # Expected counts based on Benford's Law\n    n = len(first_digits)\n    expected = benford_probs * n\n\n    # Calculate Chi-squared statistic, handling zero values\n    expected_safe = expected + 1e-10\n    chi2_stat = np.sum((observed - expected)**2 / expected_safe)\n\n    return chi2_stat\n\n# --- Add the new feature column to the DataFrame ---\n# df['benford_chi2'] = df.apply(calculate_benford_chi2, axis=1)\n","metadata":{"id":"CfQi6LGLu66u","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T16:06:08.804702Z","iopub.execute_input":"2025-10-12T16:06:08.804978Z","iopub.status.idle":"2025-10-12T16:06:08.811518Z","shell.execute_reply.started":"2025-10-12T16:06:08.804958Z","shell.execute_reply":"2025-10-12T16:06:08.810777Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Separate all normal and fraud data\ndf_normal = df[df['misstate'] == 0]\ndf_fraud = df[df['misstate'] == 1]\n\n# Define features\nfeatures = ['act', 'ap', 'at', 'ceq', 'che', 'cogs', 'csho', 'dlc', 'dltis', 'dltt', 'dp', \n            'ib', 'invt', 'ivao', 'ivst', 'lct', 'lt', 'ni', 'ppegt', 'pstk', 're', 'rect', 'sale',\n            'sstk', 'txp', 'txt', 'xint', 'prcc_f']\n\n# Split the NORMAL data first into a training and a testing set\nX_normal = df_normal[features].dropna()\ny_normal = df_normal.loc[X_normal.index]['misstate']\n\nX_train, X_test_normal, y_train, y_test_normal = train_test_split(\n    X_normal, y_normal, test_size=0.05, random_state=42\n)\n\nlower_quantile = 0.01\nupper_quantile = 0.99\n# Store the bounds learned from the training data\nbounds = {}\nfor col in X_train.columns:\n    lower_bound = X_train[col].quantile(lower_quantile)\n    upper_bound = X_train[col].quantile(upper_quantile)\n    bounds[col] = (lower_bound, upper_bound)\n    # Clip the training data\n    X_train[col] = X_train[col].clip(lower_bound, upper_bound)\n\n# Clip the test data using the bounds from the TRAINING data\nfor col in X_test_normal.columns:\n    lower_bound, upper_bound = bounds[col]\n    X_test_normal[col] = X_test_normal[col].clip(lower_bound, upper_bound)\n\npt = PowerTransformer(method='yeo-johnson')\nscaler = StandardScaler()\n\nX_train_transformed = pt.fit_transform(X_train)\nX_train_scaled = scaler.fit_transform(X_train_transformed)\n\nzero_variance_cols = X_train.columns[scaler.scale_ == 0]\nif len(zero_variance_cols) > 0:\n    print(\"Found columns with zero variance (standard deviation):\")\n    print(list(zero_variance_cols))\nelse:\n    print(\"No columns with zero variance found.\")\n\n# Prepare the fraud data\nX_test_fraud = df_fraud[features].dropna()\ny_test_fraud = df_fraud.loc[X_test_fraud.index]['misstate']\n\n# clip the fraud data using the bounds learned from the TRAINING data\nfor col in X_test_fraud.columns:\n    if col in bounds:\n        lower_bound, upper_bound = bounds[col]\n        X_test_fraud[col] = X_test_fraud[col].clip(lower_bound, upper_bound)\n\n# Transform normal test data and fraud test data\nX_test_normal_transformed = pt.transform(X_test_normal)\nX_test_normal_scaled = scaler.transform(X_test_normal_transformed)\n\nX_test_fraud_transformed = pt.transform(X_test_fraud)\nX_test_fraud_scaled = scaler.transform(X_test_fraud_transformed)\n\nX_test_scaled = np.vstack([X_test_fraud_scaled, X_test_normal_scaled])\ny_test = pd.concat([y_test_fraud, y_test_normal])\n\nprint(\"\\nPreprocessing complete.\")\nprint(f\"Shape of the final scaled training data (X_train_scaled): {X_train_scaled.shape}\")\nprint(f\"Shape of the final scaled test data (X_test_scaled): {X_test_scaled.shape}\")\nprint(f\"Shape of the final test labels (y_test): {y_test.shape}\")","metadata":{"id":"RmAJrI9uzvhl","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:18:38.737081Z","iopub.execute_input":"2025-10-12T18:18:38.737368Z","iopub.status.idle":"2025-10-12T18:18:41.334208Z","shell.execute_reply.started":"2025-10-12T18:18:38.737348Z","shell.execute_reply":"2025-10-12T18:18:41.332998Z"}},"outputs":[{"name":"stdout","text":"No columns with zero variance found.\n\nPreprocessing complete.\nShape of the final scaled training data (X_train_scaled): (137826, 28)\nShape of the final scaled test data (X_test_scaled): (8219, 28)\nShape of the final test labels (y_test): (8219,)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"#pd.DataFrame(X_train_transformed).skew()\nfrom scipy.stats import energy_distance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nmean_normal = np.mean(X_train_scaled, axis=0)\nmean_fraud = np.mean(X_test_fraud_scaled, axis=0)\n\nprint(f\"Mean of Normal Class (shape {mean_normal.shape}):\\n{mean_normal}\")\nprint(f\"Mean of Fraud Class (shape {mean_fraud.shape}):\\n{mean_fraud}\")\n\ncov_normal = np.cov(X_train_scaled, rowvar=False)\ncov_fraud = np.cov(X_test_fraud_scaled, rowvar=False)\n\n\"\"\"\nprint(\"\\n--- Covariance Matrices ---\")\nprint(f\"Covariance of Normal Class (shape {cov_normal.shape}):\\n{cov_normal}\")\nprint(f\"Covariance of Fraud Class (shape {cov_fraud.shape}):\\n{cov_fraud}\")\n\ndistance = energy_distance(X_train_scaled, X_test_fraud_scaled)\n\nprint(f\"\\n--- Energy Distance ---\")\nprint(f\"The Energy Distance between the normal and fraud classes is: {distance:.4f}\")\n\"\"\"\n\nR_X_combined = np.vstack([X_train_scaled, X_test_fraud_scaled])\n\n# Create corresponding labels: 0 for normal, 1 for fraud\nR_y_normal = np.zeros(X_train_scaled.shape[0])\nR_y_fraud = np.ones(X_test_fraud_scaled.shape[0])\nR_y_combined = np.concatenate([R_y_normal, R_y_fraud])\n\nR_X_train, R_X_test, R_y_train, R_y_test = train_test_split(\n    R_X_combined, R_y_combined, test_size=0.5, stratify=R_y_combined, random_state=42\n)\n\n# Use a robust classifier like a Random Forest\nR_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nR_classifier.fit(R_X_train, R_y_train)\n\nR_probs = R_classifier.predict_proba(R_X_test)[:, 1]\n\n# Calculate the Area Under the ROC Curve (AUC)\nR_auc_score = roc_auc_score(R_y_test, R_probs)\n\nprint(f\"\\n--- Classifier Two-Sample Test ---\")\nprint(f\"AUC Score: {R_auc_score:.4f}\")\n\nif R_auc_score > 0.95:\n    print(\"Interpretation: The distributions are very different (easily separable).\")\nelif R_auc_score > 0.6:\n    print(\"Interpretation: The distributions are noticeably different.\")\nelse:\n    print(\"Interpretation: The distributions are very similar (hard to separate).\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:18:44.840964Z","iopub.execute_input":"2025-10-12T18:18:44.842066Z","iopub.status.idle":"2025-10-12T18:18:58.069366Z","shell.execute_reply.started":"2025-10-12T18:18:44.842033Z","shell.execute_reply":"2025-10-12T18:18:58.068586Z"}},"outputs":[{"name":"stdout","text":"Mean of Normal Class (shape (28,)):\n[-1.74251190e-17 -1.14449006e-17  1.35070449e-17 -1.23728655e-17\n  1.60847252e-17  8.76411309e-19  5.56778949e-18  1.35070449e-17\n  2.62923393e-17  2.94886629e-17 -1.67033685e-17  3.29943081e-18\n  3.95931697e-17  1.36101521e-17  2.18587291e-17  1.44350098e-17\n -1.60847252e-17 -3.29943081e-18  1.27337408e-17  3.09321638e-18\n  1.75282262e-18  1.75282262e-18 -2.24773724e-17  1.64971541e-18\n  2.84575907e-17  1.00013996e-17  2.48488383e-17 -3.57782029e-17]\nMean of Fraud Class (shape (28,)):\n[ 0.62686782  0.54287647  0.57004248  0.35583408  0.47504417  0.5481387\n  0.3458107   0.33785814  0.40297866  0.39368974  0.45286221 -0.03604202\n  0.42054699  0.2645479   0.16025253  0.58386885  0.53466111 -0.05750773\n  0.3604388   0.02490725  0.03633897  0.62992805  0.57971417  0.56922949\n  0.32189184  0.12910228  0.39756763  0.59001675]\n\n--- Classifier Two-Sample Test ---\nAUC Score: 0.7919\nInterpretation: The distributions are noticeably different.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(f\"Original training data shape: {X_train_scaled.shape}\")\n\n# Define a threshold for extreme values\nthreshold = 200\n\n# Find the indices of all rows that contain a value exceeding the threshold\noutlier_indices = np.where(np.abs(X_train_scaled) > threshold)[0]\noutlier_indices = np.unique(outlier_indices)\n\nprint(f\"Found {len(outlier_indices)} rows with extreme outlier values.\")\nX_train_scaled = np.delete(X_train_scaled, outlier_indices, axis=0)\nprint(f\"New training data shape after filtering: {X_train_scaled.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYmbSvDKzR9L","outputId":"7755bf33-ce3d-4fd1-b842-1009b54c31f0","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:18:58.113440Z","iopub.execute_input":"2025-10-12T18:18:58.113663Z","iopub.status.idle":"2025-10-12T18:18:58.153938Z","shell.execute_reply.started":"2025-10-12T18:18:58.113645Z","shell.execute_reply":"2025-10-12T18:18:58.153095Z"}},"outputs":[{"name":"stdout","text":"Original training data shape: (137826, 28)\nFound 0 rows with extreme outlier values.\nNew training data shape after filtering: (137826, 28)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.SiLU(),\n            nn.Linear(32, encoding_dim),\n            nn.SiLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, 32),\n            nn.SiLU(),\n            nn.Linear(32, input_dim)\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n# --- Autoencoder Training ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ninput_dim = X_train_scaled.shape[1]\nencoding_dim = 16 # Hyperparameter: dimension of the compressed representation\nae_epochs = 100\nae_batch_size = 256\nae_learning_rate = 1e-3\n\nautoencoder = Autoencoder(input_dim, encoding_dim).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(autoencoder.parameters(), lr=ae_learning_rate)\n\nX_train_tensor = torch.from_numpy(X_train_scaled.astype(np.float32))\ntrain_dataset_ae = TensorDataset(X_train_tensor, X_train_tensor) # Input and target are the same\ntrain_loader_ae = DataLoader(train_dataset_ae, batch_size=ae_batch_size, shuffle=True)\n\nprint(\"Starting Autoencoder training...\")\nfor epoch in range(ae_epochs):\n    autoencoder.train()\n    train_loss = 0.0\n    for data in train_loader_ae:\n        inputs, targets = data\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = autoencoder(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader_ae)\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch + 1}/{ae_epochs}, Reconstruction Loss: {avg_train_loss:.6f}\")\n\nprint(\"Autoencoder training finished.\")\n\n# --- Extract Features using the Trained Encoder ---\nencoder = autoencoder.encoder\nencoder.eval()\n\nwith torch.no_grad():\n    X_train_encoded_tensor = encoder(torch.from_numpy(X_train_scaled.astype(np.float32)).to(device))\n    X_test_encoded_tensor = encoder(torch.from_numpy(X_test_scaled.astype(np.float32)).to(device))\n\nX_train_encoded = X_train_encoded_tensor.cpu().numpy()\nX_test_encoded = X_test_encoded_tensor.cpu().numpy()\n\nprint(f\"Original feature dimension: {input_dim}\")\nprint(f\"New encoded feature dimension: {X_train_encoded.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:34:23.320652Z","iopub.execute_input":"2025-10-12T18:34:23.320961Z","iopub.status.idle":"2025-10-12T18:37:19.290815Z","shell.execute_reply.started":"2025-10-12T18:34:23.320940Z","shell.execute_reply":"2025-10-12T18:37:19.290028Z"}},"outputs":[{"name":"stdout","text":"Starting Autoencoder training...\nEpoch 10/100, Reconstruction Loss: 0.022433\nEpoch 20/100, Reconstruction Loss: 0.019124\nEpoch 30/100, Reconstruction Loss: 0.017771\nEpoch 40/100, Reconstruction Loss: 0.017175\nEpoch 50/100, Reconstruction Loss: 0.016848\nEpoch 60/100, Reconstruction Loss: 0.016539\nEpoch 70/100, Reconstruction Loss: 0.016217\nEpoch 80/100, Reconstruction Loss: 0.015818\nEpoch 90/100, Reconstruction Loss: 0.015455\nEpoch 100/100, Reconstruction Loss: 0.015236\nAutoencoder training finished.\nOriginal feature dimension: 28\nNew encoded feature dimension: 16\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Convert to PyTorch Tensors\nX_train_tensor = torch.from_numpy(X_train_encoded.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test_encoded.astype(np.float32))\n\n# Check for NaNs and infinite values in tensors\nif torch.isnan(X_train_tensor).any() or torch.isinf(X_train_tensor).any():\n    print(\"NaN or infinite values found in X_train_tensor!\")\nif torch.isnan(y_train_tensor).any() or torch.isinf(y_train_tensor).any():\n    print(\"NaN or infinite values found in y_train_tensor!\")\nif torch.isnan(X_test_tensor).any() or torch.isinf(X_test_tensor).any():\n    print(\"NaN or infinite values found in X_test_tensor!\")\n\n# Create datasets from the scaled tensors\ntrain_dataset = TensorDataset(X_train_tensor)\n\n# Create a validation split\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\nclass ContextIgnoredMLP(nn.Module):\n    def __init__(self, in_features, out_features, hidden_features):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(in_features, hidden_features),\n            nn.SiLU(),\n            nn.Linear(hidden_features, hidden_features),\n            nn.SiLU(),\n            nn.Linear(hidden_features, out_features),\n        )\n\n    def forward(self, inputs, context=None):\n        # We simply ignore the context and pass the inputs to our network\n        return self.network(inputs)\n\n#from kan import KAN # (pip install pykan)\nfrom efficient_kan import KAN as EfficientKAN\n\nclass ContextIgnoredKAN(nn.Module):\n    \"\"\"\n    A wrapper for a pykan.KAN model to make it compatible with nflows.\n    It accepts and ignores the 'context' argument in its forward pass.\n    \"\"\"\n    def __init__(self, in_features, out_features, hidden_layers=[128, 128], grid_size=3, spline_order=3):\n        super().__init__()\n        \n        # Define the KAN architecture\n        # width = [input_dim, hidden1_dim, hidden2_dim, ..., output_dim]\n        width = [in_features, *hidden_layers, out_features]\n        \n        self.kan = EfficientKAN(\n            width,\n            grid_size=grid_size,\n            spline_order=spline_order\n        )\n\n    def forward(self, inputs, context=None):\n        # We simply ignore the context and pass the inputs to our KAN\n        return self.kan(inputs)\n        \ndef weights_init(m):\n    \"\"\"\n    Applies Kaiming (He) normal initialization to linear layers.\n    This helps prevent gradients from exploding or vanishing early in training.\n    \"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\n# --- Define the Flow-based Model using nflows ---\ndef create_nflows_model(input_dim, num_layers=6, hidden_dim=256, device='cpu'):\n    \"\"\"Creates a RealNVP-style model using the nflows library.\"\"\"\n    base_dist = StandardNormal(shape=[input_dim])\n\n    transform_net_create_fn = lambda in_features, out_features: ContextIgnoredKAN(\n            in_features=in_features,\n            out_features=out_features,\n            hidden_layers=[hidden_dim, hidden_dim] \n    )\n\n    def create_transform_net(in_features, out_features):\n        network = ContextIgnoredKAN(\n            in_features=in_features,\n            out_features=out_features,\n            hidden_layers=[hidden_dim, hidden_dim]\n        )\n        \n        # ROBUST INITIALIZATION\n        final_kan_layer = network.kan.layers[-1]\n        torch.nn.init.normal_(final_kan_layer.base_weight, mean=0.0, std=0.01)\n        torch.nn.init.normal_(final_kan_layer.spline_weight, mean=0.0, std=0.01)\n        \n        return network\n    \n    \"\"\"\n    transform_net_create_fn = lambda in_features, out_features: ContextIgnoredMLP(\n        in_features=in_features,\n        out_features=out_features,\n        hidden_features=hidden_dim\n    )\n    \"\"\"\n    transforms = []\n    mask = create_alternating_binary_mask(features=input_dim, even=True)\n    for _ in range(num_layers):\n        transforms.append(RandomPermutation(features=input_dim))\n        transforms.append(ActNorm(features=input_dim))\n        transforms.append(\n            AffineCouplingTransform(\n                mask=mask,\n                transform_net_create_fn=create_transform_net\n            )\n        )\n        mask = 1 - mask # Flip the mask for the next layer\n        \n        \"\"\"\n        transforms.append(\n            MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n                features=input_dim,\n                hidden_features=hidden_dim,\n                # These are sensible defaults for the spline transform\n                num_bins=8,\n                tails='linear',\n                tail_bound=5,\n            )\n        )\n        \"\"\"\n\n    transform = CompositeTransform(transforms)\n    flow = Flow(transform, base_dist)\n    return flow","metadata":{"id":"ptzCE-UWPrOI","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:37:22.957097Z","iopub.execute_input":"2025-10-12T18:37:22.957378Z","iopub.status.idle":"2025-10-12T18:37:22.984603Z","shell.execute_reply.started":"2025-10-12T18:37:22.957351Z","shell.execute_reply":"2025-10-12T18:37:22.983980Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# --- Forensic Debugging Cell ---\nprint(\"--- Starting Forensic Debugging ---\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntrain_dataset = TensorDataset(X_train_tensor)\n\n# Use the exact same (simplified) settings that are failing\ninput_dim = X_train_encoded.shape[1]\nnum_coupling_layers = 12\nhidden_dim = 256\nbatch_size = 256 \n\n# Re-create the DataLoader to ensure we get a consistent first batch\n# NOTE: Set shuffle=False to make the test repeatable.\ntrain_loader_debug = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\n# Get the VERY FIRST batch of data\ntry:\n    first_batch = next(iter(train_loader_debug))[0]\n    print(f\"\\nSuccessfully loaded one batch of size: {first_batch.shape}\")\nexcept StopIteration:\n    print(\"Error: DataLoader is empty. Check your train_dataset.\")\n    # Stop here if there's no data\n\n# Inspect the batch itself for any issues post-scaling and loading\nprint(f\"Batch Stats: Min={first_batch.min():.4f}, Max={first_batch.max():.4f}, Mean={first_batch.mean():.4f}\")\nif torch.isnan(first_batch).any() or torch.isinf(first_batch).any():\n    print(\"!!! FATAL ERROR: The batch itself contains NaNs or Infs before entering the model!\")\nelse:\n    print(\"Batch data appears clean (no NaNs or Infs).\")\n\n# Create a fresh model instance and run a single forward pass\nmodel_debug = create_nflows_model(input_dim, num_coupling_layers, hidden_dim)\nmodel_debug.to(device)\nfirst_batch = first_batch.to(device)\n#model_debug.double()\nmodel_debug.apply(weights_init)\nmodel_debug.eval() # Set to evaluation mode for a clean forward pass\n\nprint(\"\\n--- Running Forward Pass on Single Batch ---\")\nwith torch.no_grad():\n    try:\n        log_probs = model_debug.log_prob(inputs=first_batch)\n\n        # 6. Check for issues in the model's output\n        nan_mask = torch.isnan(log_probs)\n        inf_mask = torch.isinf(log_probs)\n\n        if nan_mask.any():\n            print(f\"FAILURE: Found {nan_mask.sum()} NaN(s) in the log_prob output!\")\n            # Find the specific row(s) in the batch that caused the NaN\n            problem_indices = nan_mask.nonzero(as_tuple=True)[0]\n            print(f\"Data row at index {problem_indices[0]} in the batch caused the NaN.\")\n            print(\"\\n--- Problematic Data Point ---\")\n            print(first_batch[problem_indices[0]])\n            print(\"------------------------------\")\n\n        elif inf_mask.any():\n            print(f\"FAILURE: Found {inf_mask.sum()} Inf(s) in the log_prob output!\")\n            problem_indices = inf_mask.nonzero(as_tuple=True)[0]\n            print(f\"Data row at index {problem_indices[0]} in the batch caused the Inf.\")\n            print(\"\\n--- Problematic Data Point ---\")\n            print(first_batch[problem_indices[0]])\n            print(\"------------------------------\")\n        else:\n            print(f\"SUCCESS: The forward pass is clean.\")\n            print(f\"Log Probs Stats: Min={log_probs.min():.4f}, Max={log_probs.max():.4f}\")\n            print(\"This suggests the NaN may be occurring in the backward pass (gradient calculation).\")\n\n\n    except Exception as e:\n        print(f\"\\nAn exception occurred during the forward pass: {e}\")\n\n# Clean up to avoid interfering with the main training loop\ndel model_debug, train_loader_debug","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlG0yQdBxyGX","outputId":"d13d4d78-c091-4668-e60f-f79a274cd2c5","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:37:31.341161Z","iopub.execute_input":"2025-10-12T18:37:31.341855Z","iopub.status.idle":"2025-10-12T18:37:31.653758Z","shell.execute_reply.started":"2025-10-12T18:37:31.341830Z","shell.execute_reply":"2025-10-12T18:37:31.652988Z"}},"outputs":[{"name":"stdout","text":"--- Starting Forensic Debugging ---\n\nSuccessfully loaded one batch of size: torch.Size([256, 16])\nBatch Stats: Min=0.5725, Max=11.5304, Mean=3.4911\nBatch data appears clean (no NaNs or Infs).\n\n--- Running Forward Pass on Single Batch ---\nSUCCESS: The forward pass is clean.\nLog Probs Stats: Min=-115.7932, Max=-38.8974\nThis suggests the NaN may be occurring in the backward pass (gradient calculation).\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import torch.nn as nn\n\n# --- Training the Model ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ninput_dim = X_train_encoded.shape[1]\nnum_coupling_layers = 24\nhidden_dim = 256\nlearning_rate = 1e-3\nbatch_size = 512\nepochs = 100\npatience = 10 # For early stopping\nbest_model_path = \"best_model.pth\"\nweight_decay = 1e-5\n\nif torch.isnan(torch.tensor(X_train_scaled)).any():\n    raise ValueError(\"Input data contains NaN values. Please clean the data before training.\")\nelse:\n    print(\"Data integrity check passed: No NaN values found in input data.\")\n\nmodel = create_nflows_model(input_dim, num_coupling_layers, hidden_dim)\n#model.double()\nmodel.apply(weights_init)\n\nif torch.cuda.device_count() > 1:\n    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\nmodel.to(device)\nprint(f\"Using device: {device}\")\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Create DataLoaders for train and validation sets\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\nprint(\"Starting model training...\")\n\nbest_val_loss = float('inf')\nearly_stopping_counter = 0\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    for (batch,) in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        if isinstance(model, nn.DataParallel):\n            loss = -model.module.log_prob(inputs=batch).mean()\n        else:\n            loss = -model.log_prob(inputs=batch).mean()\n\n        if torch.isnan(loss):\n            print(\"NaN loss detected. Stopping training.\")\n            break\n\n        loss.backward()\n\n        # gradient clipping for stabilization\n        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n\n        optimizer.step()\n        train_loss += loss.item()\n\n    if torch.isnan(loss): # Break outer loop if NaN was detected\n        break\n\n    avg_train_loss = train_loss / len(train_loader)\n\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for (batch,) in val_loader:\n            batch = batch.to(device)\n            if isinstance(model, nn.DataParallel):\n                loss = -model.module.log_prob(inputs=batch).mean()\n            else:\n                loss = -model.log_prob(inputs=batch).mean()\n            val_loss += loss.item()\n    avg_val_loss = val_loss / len(val_loader)\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n    if avg_val_loss is None or torch.isnan(torch.tensor(avg_val_loss)):\n        print(f\"Validation loss is None at epoch {epoch + 1}. Stopping training.\")\n        break\n\n    # Early stopping and best model saving\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        state_to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n        torch.save(state_to_save, best_model_path)\n        early_stopping_counter = 0\n        print(f\"  -> New best model saved with val_loss: {best_val_loss:.4f}\")\n    else:\n        early_stopping_counter += 1\n\n    if early_stopping_counter >= patience:\n        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n        break\n\nprint(\"\\nTraining finished.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnCcQQd9uE4V","outputId":"3d9ce530-6002-4912-d458-22fc0af84e42","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:37:36.438375Z","iopub.execute_input":"2025-10-12T18:37:36.438958Z","iopub.status.idle":"2025-10-12T19:26:49.200469Z","shell.execute_reply.started":"2025-10-12T18:37:36.438935Z","shell.execute_reply":"2025-10-12T19:26:49.199695Z"}},"outputs":[{"name":"stdout","text":"Data integrity check passed: No NaN values found in input data.\nUsing device: cuda\nStarting model training...\n  -> New best model saved with val_loss: 7.0209\n  -> New best model saved with val_loss: 3.3223\n  -> New best model saved with val_loss: 2.3101\n  -> New best model saved with val_loss: 1.3013\n  -> New best model saved with val_loss: 0.6008\n  -> New best model saved with val_loss: 0.1668\n  -> New best model saved with val_loss: -0.2616\nEpoch 10, Train Loss: -0.9651, Val Loss: -0.7678\n  -> New best model saved with val_loss: -0.7678\n  -> New best model saved with val_loss: -0.8860\n  -> New best model saved with val_loss: -1.8511\n  -> New best model saved with val_loss: -2.0219\n  -> New best model saved with val_loss: -2.1711\n  -> New best model saved with val_loss: -2.3854\n  -> New best model saved with val_loss: -2.9258\nEpoch 20, Train Loss: -2.8159, Val Loss: -1.7922\n  -> New best model saved with val_loss: -3.2537\n  -> New best model saved with val_loss: -3.2754\nEpoch 30, Train Loss: -3.6218, Val Loss: -3.6195\n  -> New best model saved with val_loss: -3.6195\n  -> New best model saved with val_loss: -3.7433\n  -> New best model saved with val_loss: -3.8912\n  -> New best model saved with val_loss: -4.4608\nEpoch 40, Train Loss: -4.2300, Val Loss: -4.1510\n  -> New best model saved with val_loss: -4.5339\n  -> New best model saved with val_loss: -4.7845\nEpoch 50, Train Loss: -4.6498, Val Loss: -3.8079\n\nEarly stopping triggered after 54 epochs.\n\nTraining finished.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nflow_model = create_nflows_model(input_dim, num_coupling_layers, hidden_dim)\n\nif os.path.exists(best_model_path):\n    print(f\"\\nLoading best model from '{best_model_path}' for evaluation.\")\n    state_dict = torch.load(best_model_path, map_location=device)\n\n    # Handle the 'module.' prefix if the model was saved using DataParallel\n    if next(iter(state_dict)).startswith('module.'):\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:] # remove `module.`\n            new_state_dict[name] = v\n        flow_model.load_state_dict(new_state_dict)\n    else:\n        flow_model.load_state_dict(state_dict)\nelse:\n    print(\"\\nWarning: No saved model found. Cannot perform evaluation.\")\n    exit()\n    \nflow_model.to(device)\nflow_model.eval()\n\ndef detect_anomalies(model, data_loader, full_train_data_tensor, threshold_percentile=5):\n    \"\"\"Calculates log probabilities and flags anomalies.\"\"\"\n    model.eval()\n    log_probs_list = []\n\n    with torch.no_grad():\n        for (batch,) in data_loader:\n            batch = batch.to(device)\n            log_probs = model.log_prob(inputs=batch)\n            log_probs_list.append(log_probs.cpu())\n\n    log_probs_tensor = torch.cat(log_probs_list)\n\n    # Use the full training data to set a robust threshold\n    with torch.no_grad():\n        train_log_probs = model.log_prob(inputs=full_train_data_tensor.to(device))\n\n    threshold = np.percentile(train_log_probs.cpu().numpy(), threshold_percentile)\n    print(f\"\\nAnomaly threshold (log probability): {threshold:.4f}\")\n\n    predictions = log_probs_tensor.numpy() < threshold\n    return predictions, log_probs_tensor.numpy()\n\ntest_dataset_encoded = TensorDataset(torch.from_numpy(X_test_encoded.astype(np.float32)))\ntest_loader_encoded = DataLoader(test_dataset_encoded, batch_size=batch_size)\n\nfull_train_tensor_encoded = torch.from_numpy(X_train_encoded.astype(np.float32))\n\nanomaly_predictions, log_probabilities = detect_anomalies(flow_model, test_loader_encoded, full_train_tensor_encoded, threshold_percentile=5)\n\n# --- Performance Metrics ---\ncorrect_predictions = (anomaly_predictions == y_test)\naccuracy = np.mean(correct_predictions)\ntrue_positives = np.sum(anomaly_predictions & (y_test == 1))\nfalse_positives = np.sum(anomaly_predictions & (y_test == 0))\nrecall = true_positives / np.sum(y_test == 1) if np.sum(y_test == 1) > 0 else 0\n\nprint(\"\\n--- Performance Metrics ---\")\nprint(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Fraud cases detected (Recall): {recall * 100:.2f}% ({true_positives}/{np.sum(y_test==1)})\")\nprint(f\"Normal cases flagged as fraud (False Positives): {false_positives}/{np.sum(y_test==0)}\")\n\nprint(\"\\n--- Classification Report ---\")\nprint(classification_report(y_test, anomaly_predictions, target_names=['Normal', 'Fraud']))\n\nauc_score = roc_auc_score(y_test, 1 - log_probabilities) # Use 1-prob for AUC scoring\nprint(f\"ROC AUC Score: {auc_score:.4f}\")\n\ncm = confusion_matrix(y_test, anomaly_predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\nplt.title('Confusion Matrix')\nplt.ylabel('Actual Class')\nplt.xlabel('Predicted Class')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T19:46:09.844730Z","iopub.execute_input":"2025-10-12T19:46:09.845301Z","iopub.status.idle":"2025-10-12T19:46:17.997073Z","shell.execute_reply.started":"2025-10-12T19:46:09.845276Z","shell.execute_reply":"2025-10-12T19:46:17.996186Z"}},"outputs":[{"name":"stdout","text":"\nLoading best model from 'best_model.pth' for evaluation.\n\nAnomaly threshold (log probability): -9.1820\n\n--- Performance Metrics ---\nOverall Accuracy: 82.94%\nFraud cases detected (Recall): 14.00% (135/964)\nNormal cases flagged as fraud (False Positives): 573/7255\n\n--- Classification Report ---\n              precision    recall  f1-score   support\n\n      Normal       0.89      0.92      0.91      7255\n       Fraud       0.19      0.14      0.16       964\n\n    accuracy                           0.83      8219\n   macro avg       0.54      0.53      0.53      8219\nweighted avg       0.81      0.83      0.82      8219\n\nROC AUC Score: 0.6250\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":42}]}
{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import copy\n",
        "\n",
        "# --- Framework-agnostic Data Generation Code ---\n",
        "class StockOracle:\n",
        "    \"\"\"\n",
        "    A class to simulate a stock's price using various stochastic processes.\n",
        "    This version is simplified for generating clean datasets for the ML model.\n",
        "    \"\"\"\n",
        "    def __init__(self, periods=252, x0=100, kappa=0.5, theta=100, sigma=0.5,\n",
        "                 pump_start_percent=0.4, pump_end_percent=0.6, dump_end_percent=0.7,\n",
        "                 post_dump_drift_strength=0.1, simulation_type=\"pump_and_dump\", trend_drift=0.5):\n",
        "        self.periods = periods\n",
        "        self.x0 = x0\n",
        "        self.kappa = kappa\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.pump_start_percent = pump_start_percent\n",
        "        self.pump_end_percent = pump_end_percent\n",
        "        self.dump_end_percent = dump_end_percent\n",
        "        self.post_dump_drift_strength = post_dump_drift_strength\n",
        "        self.simulation_type = simulation_type\n",
        "        self.trend_drift = trend_drift\n",
        "\n",
        "        if self.simulation_type == \"pump_and_dump\":\n",
        "            self.price_history = self._simulate_pump_dump_data()\n",
        "        elif self.simulation_type == \"mean_reverting\":\n",
        "            self.price_history = self._simulate_mean_reverting_data()\n",
        "        elif self.simulation_type == \"trend\":\n",
        "            self.price_history = self._simulate_trend_data()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid simulation_type provided.\")\n",
        "\n",
        "    def _simulate_pump_dump_data(self):\n",
        "        s = [self.x0]\n",
        "        dt = 1.0 / self.periods\n",
        "        pump_start_idx = int(self.periods * self.pump_start_percent)\n",
        "        pump_end_idx = int(self.periods * self.pump_end_percent)\n",
        "        dump_end_idx = int(self.periods * self.dump_end_percent)\n",
        "\n",
        "        for t in range(1, self.periods):\n",
        "            s_t_minus_1 = s[t - 1]\n",
        "            drift = self.kappa * (self.theta - s_t_minus_1) * dt\n",
        "            volatility = s_t_minus_1 * self.sigma * math.sqrt(dt) * random.gauss(0, 1)\n",
        "            additional_drift = 0\n",
        "\n",
        "            if pump_start_idx <= t < pump_end_idx:\n",
        "                additional_drift = 2.0 * dt * self.x0\n",
        "            elif pump_end_idx <= t < dump_end_idx:\n",
        "                additional_drift = -5.0 * dt * self.x0\n",
        "            elif t >= dump_end_idx:\n",
        "                additional_drift = -self.post_dump_drift_strength * (s_t_minus_1 - self.x0) * dt\n",
        "\n",
        "            s_t = (s_t_minus_1 + drift + volatility + additional_drift)\n",
        "            s.append(max(0, s_t))\n",
        "        return np.array(s)\n",
        "\n",
        "    def _simulate_mean_reverting_data(self):\n",
        "        s = [self.x0]\n",
        "        dt = 1.0 / self.periods\n",
        "        for t in range(1, self.periods):\n",
        "            s_t_minus_1 = s[t - 1]\n",
        "            drift = self.kappa * (self.theta - s_t_minus_1) * dt\n",
        "            volatility = s_t_minus_1 * self.sigma * math.sqrt(dt) * random.gauss(0, 1)\n",
        "            s_t = (s_t_minus_1 + drift + volatility)\n",
        "            s.append(max(0, s_t))\n",
        "        return np.array(s)\n",
        "\n",
        "    def _simulate_trend_data(self):\n",
        "        s = [self.x0]\n",
        "        dt = 1.0 / self.periods\n",
        "        for t in range(1, self.periods):\n",
        "            s_t_minus_1 = s[t - 1]\n",
        "            drift = self.trend_drift * dt * self.x0\n",
        "            volatility = s_t_minus_1 * self.sigma * math.sqrt(dt) * random.gauss(0, 1)\n",
        "            s_t = (s_t_minus_1 + drift + volatility)\n",
        "            s.append(max(0, s_t))\n",
        "        return np.array(s)\n",
        "\n",
        "def generate_dataset(num_samples=10000, periods=252):\n",
        "    \"\"\"\n",
        "    Generates a synthetic dataset for classification.\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Generate Pump-and-Dump samples (Label 1)\n",
        "    for _ in range(num_samples // 2):\n",
        "        oracle = StockOracle(\n",
        "            periods=periods,\n",
        "            x0=random.uniform(50, 150),\n",
        "            kappa=random.uniform(0.1, 1.0),\n",
        "            theta=random.uniform(50, 150),\n",
        "            sigma=random.uniform(0.1, 1.0),\n",
        "            pump_start_percent=random.uniform(0.1, 0.4),\n",
        "            pump_end_percent=random.uniform(0.5, 0.7),\n",
        "            dump_end_percent=random.uniform(0.7, 0.9),\n",
        "            simulation_type=\"pump_and_dump\"\n",
        "        )\n",
        "        X.append(oracle.price_history)\n",
        "        y.append(1)\n",
        "\n",
        "    # Generate \"Normal\" market samples (Label 0)\n",
        "    for _ in range(num_samples // 4):\n",
        "        oracle = StockOracle(\n",
        "            periods=periods,\n",
        "            x0=random.uniform(50, 150),\n",
        "            kappa=random.uniform(0.1, 1.0),\n",
        "            theta=random.uniform(50, 150),\n",
        "            sigma=random.uniform(0.1, 1.0),\n",
        "            simulation_type=\"mean_reverting\"\n",
        "        )\n",
        "        X.append(oracle.price_history)\n",
        "        y.append(0)\n",
        "\n",
        "    for _ in range(num_samples // 4):\n",
        "        oracle = StockOracle(\n",
        "            periods=periods,\n",
        "            x0=random.uniform(50, 150),\n",
        "            sigma=random.uniform(0.1, 1.0),\n",
        "            trend_drift=random.uniform(-0.5, 0.5),\n",
        "            simulation_type=\"trend\"\n",
        "        )\n",
        "        X.append(oracle.price_history)\n",
        "        y.append(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# --- PyTorch Specific Code ---\n",
        "\n",
        "class TimeseriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for time series data.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class PumpDumpClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    An LSTM-based classifier for pump-and-dump events.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
        "        super(PumpDumpClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(hidden_size, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, input_size)\n",
        "        out, _ = self.lstm(x)\n",
        "        # Take the output from the last timestep\n",
        "        out = out[:, -1, :]\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "pegm0sXGetLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_SAMPLES = 20_000\n",
        "PERIODS = 252 # Number of trading days in a year\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "INPUT_SIZE = 1\n",
        "HIDDEN_SIZE = 64\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "# Early stopping parameters\n",
        "PATIENCE = 5\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"Generating synthetic data...\")\n",
        "X, y = generate_dataset(num_samples=NUM_SAMPLES, periods=PERIODS)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42, stratify=y_train_val)\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "train_dataset = TimeseriesDataset(X_train.reshape(-1, PERIODS, INPUT_SIZE), y_train)\n",
        "val_dataset = TimeseriesDataset(X_val.reshape(-1, PERIODS, INPUT_SIZE), y_val)\n",
        "test_dataset = TimeseriesDataset(X_test.reshape(-1, PERIODS, INPUT_SIZE), y_test)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"\\nTraining PyTorch LSTM model with early stopping...\")\n",
        "model = PumpDumpClassifier(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_RATE).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for i, (sequences, labels) in enumerate(train_loader):\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for sequences, labels in val_loader:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Save the best model state\n",
        "        best_model_state = copy.deepcopy(model.state_dict())\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch+1}. Restoring best model state.\")\n",
        "            model.load_state_dict(best_model_state)\n",
        "            break\n",
        "\n",
        "\n",
        "print(\"\\nEvaluating model performance...\")\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences = sequences.to(device)\n",
        "        outputs = model(sequences)\n",
        "        predicted = outputs.round()\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Pump-and-Dump\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycSh7-2He5Ly",
        "outputId": "10568a36-3fbb-466f-b564-c313857c2c57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Generating synthetic data...\n",
            "\n",
            "Training PyTorch LSTM model with early stopping...\n",
            "Epoch [1/50], Loss: 0.5364, Validation Loss: 0.6200\n",
            "Epoch [2/50], Loss: 0.4867, Validation Loss: 0.5946\n",
            "Epoch [3/50], Loss: 0.3598, Validation Loss: 0.5004\n",
            "Epoch [4/50], Loss: 0.4804, Validation Loss: 0.4739\n",
            "Epoch [5/50], Loss: 0.5391, Validation Loss: 0.5286\n",
            "Epoch [6/50], Loss: 0.5889, Validation Loss: 0.3307\n",
            "Epoch [7/50], Loss: 0.2214, Validation Loss: 0.2712\n",
            "Epoch [8/50], Loss: 0.1333, Validation Loss: 0.2538\n",
            "Epoch [9/50], Loss: 0.2604, Validation Loss: 0.3244\n",
            "Epoch [10/50], Loss: 0.2901, Validation Loss: 0.2407\n",
            "Epoch [11/50], Loss: 0.3172, Validation Loss: 0.2214\n",
            "Epoch [12/50], Loss: 0.1599, Validation Loss: 0.2286\n",
            "Epoch [13/50], Loss: 0.1105, Validation Loss: 0.2151\n",
            "Epoch [14/50], Loss: 0.3579, Validation Loss: 0.2128\n",
            "Epoch [15/50], Loss: 0.3380, Validation Loss: 0.2267\n",
            "Epoch [16/50], Loss: 0.1100, Validation Loss: 0.2272\n",
            "Epoch [17/50], Loss: 0.3213, Validation Loss: 0.2164\n",
            "Epoch [18/50], Loss: 0.1646, Validation Loss: 0.1980\n",
            "Epoch [19/50], Loss: 0.2694, Validation Loss: 0.2219\n",
            "Epoch [20/50], Loss: 0.1341, Validation Loss: 0.2080\n",
            "Epoch [21/50], Loss: 0.1800, Validation Loss: 0.2029\n",
            "Epoch [22/50], Loss: 0.2471, Validation Loss: 0.2212\n",
            "Epoch [23/50], Loss: 0.1614, Validation Loss: 0.1977\n",
            "Epoch [24/50], Loss: 0.1592, Validation Loss: 0.1937\n",
            "Epoch [25/50], Loss: 0.1657, Validation Loss: 0.1929\n",
            "Epoch [26/50], Loss: 0.2189, Validation Loss: 0.2002\n",
            "Epoch [27/50], Loss: 0.3284, Validation Loss: 0.1862\n",
            "Epoch [28/50], Loss: 0.3508, Validation Loss: 0.1897\n",
            "Epoch [29/50], Loss: 0.2527, Validation Loss: 0.1913\n",
            "Epoch [30/50], Loss: 0.0861, Validation Loss: 0.2014\n",
            "Epoch [31/50], Loss: 0.2156, Validation Loss: 0.1903\n",
            "Epoch [32/50], Loss: 0.2460, Validation Loss: 0.1964\n",
            "Early stopping at epoch 32. Restoring best model state.\n",
            "\n",
            "Evaluating model performance...\n",
            "Accuracy: 0.9227\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Normal       0.92      0.92      0.92      2000\n",
            "Pump-and-Dump       0.92      0.92      0.92      2000\n",
            "\n",
            "     accuracy                           0.92      4000\n",
            "    macro avg       0.92      0.92      0.92      4000\n",
            " weighted avg       0.92      0.92      0.92      4000\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}